{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332b5e50-a47c-4a40-a5f6-9dfe7f534914",
   "metadata": {},
   "source": [
    "## **TSFRESH Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4748408-b33a-4a2d-8e45-82b545a1a099",
   "metadata": {},
   "source": [
    "In this notebook, a template pipeline for clustering from the features extracted using the TSFRESH python package is presented. For demonstration purposes, we have used the time-series data for the calibration period ([-15, -3] from sample detect time) without standardization and without noise. reduction. \n",
    "\n",
    "Note that this same pipeline has been applied to the following windows: \n",
    "\n",
    "* calibration window ([-15, -3] from sample detect time) - without standardization\n",
    "* calibration window ([-15, -3] from sample detect time) - standardized before windowed\n",
    "* post window ([12, 16] from sample detect time) - standardized before windowed\n",
    "* sample window ([32, 35] from sample detect time) - standardized before windowed\n",
    "* whole timeseries from [-30, 40] (w.r.t sample detect time) - normalized and smoothed (using convolution with a bartlett window of len = 50)\n",
    "\n",
    "This pipeline did not allow us to cluster the ECDs. The PCA visualizations helped us notice just how similar the ECD readings were from the unsuccessful readings.\n",
    "\n",
    "1. Extract features using the `feature_matrix` function\n",
    "2. Apply PCA to these features for dimension reduction\n",
    "3. Use hierarchical cluster or Gaussian Mixture Modelling to find clusters and subclusters by applying a multi-step approach.\n",
    "    \n",
    "Throughout this notebook, visualizations are used in order to inform the user on what steps should be taken next (i.e. tuning the parameter for the number of clusters, deciding whether there seems to be clusters forming etc.), which means the process is quite subjective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13965fe6-367a-44ac-9331-f831073a354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly                                          ## Package necessary for 3D represntation of the PCA components\n",
    "import plotly.graph_objs as go\n",
    "import altair as alt\n",
    "\n",
    "from tsfresh import extract_features, select_features  ## Package necessary for extracting the features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b103a8-eceb-4a5f-b40e-4e4a2de5a7b1",
   "metadata": {},
   "source": [
    "### **1- FEATURE EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f7338-e02c-4917-946a-622d978ed38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matrix(un_data, ecd_data, scale = True):\n",
    "    \"\"\"Creates a feature matrix from the raw timeseries and stores them in a dataframe.\n",
    "    Args:\n",
    "        un_data (pandas data frame): Time series for unsuccessful readings stored in rows of dataframe, with first column being the test ID.\n",
    "        ecd_data (pandas data frame): Time series for ECD readings stored in rows of dataframe, with first column being the test ID.\n",
    "        scale (bool, optional): If True, the feature matrix is standardized (preferred when clustering).\n",
    "    Returns:\n",
    "        A new pandas data frame containing various features (columns) for each reading/timeseries (rows).\n",
    "    \"\"\"\n",
    "    # Adding labels to our timeseries\n",
    "    un_data['label'] = False\n",
    "    ecd_data['label'] = True\n",
    "    \n",
    "    # Making one dataset with unsuccessful and ECD\n",
    "    data = pd.concat([un_data, ecd_data]).reset_index(drop = True)\n",
    "    \n",
    "    # Changing format of data to fit requirements of `extract_features` function\n",
    "    melt_data = pd.melt(data, id_vars = ['TestId', 'label'], var_name = 'time')\n",
    "    melt_data['time'] = pd.to_numeric(melt_data['time'])\n",
    "    melt_data = melt_data.sort_values(by = ['TestId', 'time']).reset_index(drop = True)\n",
    "   \n",
    "    # We use squeeze to go from dataframe to series because the `select_features` function needs it to be\n",
    "    y = data[['TestId', 'label']].sort_values(by = 'TestId').set_index('TestId')\n",
    "    y_series = y.squeeze()\n",
    "    \n",
    "    # Now we want to extract features seperately for all ids\n",
    "    extracted_features = extract_features(melt_data, column_id=\"TestId\", column_sort=\"time\", column_value = 'value')\n",
    "    \n",
    "    # Remove all features containing NaN values (which were created because could not be calculated on the time series, i.e. stat too low)\n",
    "    extracted_features.dropna(axis='columns', inplace = True)\n",
    "    \n",
    "    # Select the relevant features\n",
    "    impute(extracted_features)\n",
    "    features_filtered = select_features(extracted_features, y_series)\n",
    "    \n",
    "    # Scale the feature matrix\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features_filtered)\n",
    "    \n",
    "    print(f\"The total number of features created is: {len(extracted_features.columns)}\")\n",
    "    print(f\"The total number of relevant features is: {len(features_filtered.columns)}\")\n",
    "    \n",
    "    #Convert to data frame with test ids as index and appropriate column labels\n",
    "    scaled_features  = pd.DataFrame(scaled_features, columns = features_filtered.columns).set_index(features_filtered.index)\n",
    "    \n",
    "    # Making TesId a column instead of the index\n",
    "    scaled_features = y.join(scaled_features, on='TestId').reset_index(level = 0)\n",
    "    features_filtered = y.join(features_filtered, on = 'TestId').reset_index(level = 0)\n",
    "    \n",
    "    if scale == True:\n",
    "        return scaled_features\n",
    "    else:\n",
    "        return features_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d51a20-6f1f-426b-b986-7e05a1cd2024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the timeseries data\n",
    "ecd_cal = pd.read_csv(\"/Users/neethug/Desktop/Neethu/Course/DATA599/Project/Data/data/ecd_cal.csv\")\n",
    "un_cal = pd.read_csv(\"/Users/neethug/Desktop/Neethu/Course/DATA599/Project/Data/data/un_cal.csv\")\n",
    "syn_cal = pd.read_csv(\"/Users/neethug/Desktop/Neethu/Course/DATA599/Project/Data/data/syn_cal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0cda4-01cf-4710-ac4d-b4d77b0c4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the synthetic ECDs with the wild ECDs\n",
    "ecd_cal_tot = pd.concat([ecd_cal, syn_cal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c27233-1fe3-48b3-892b-1045712f359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers (TestId 9610647, 9610462) --> Identified outliers because they were on a completely different scale compared to all the other readings\n",
    "ecd_cal_tot = ecd_cal_tot[~ecd_cal_tot['TestId'].isin([9610647, 9610462])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c012c33e-fd4c-41ef-8cc6-4d6d4be0f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features\n",
    "feat_cal = feature_matrix(un_data = un_cal, ecd_data = ecd_cal_tot, scale = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088f624-c445-471e-928f-80f76f7fa765",
   "metadata": {},
   "source": [
    "### **2- DIMENSION REDUCTION WITH PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5eb86-62ae-426a-a4bd-54e57ad0cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA on all the scaled relevant features\n",
    "pca = PCA(n_components = 0.95) # keeping 95% of the cumulative variance explained\n",
    "principalComponents = pca.fit_transform(feat_cal.drop(columns = ['label', 'TestId']))\n",
    "principalDf = pd.DataFrame(principalComponents, columns = [f\"PC_{i}\" for i in range(1, len(pca.components_) + 1)])\n",
    "principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7236cd1-4b33-4848-b1ac-8c19a4ecb9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot to see how many components to keep\n",
    "plt.plot(range(1, len(pca.components_) + 1), pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee581adc-e5e3-4a01-b03a-f4ea244112fc",
   "metadata": {},
   "source": [
    "Here, we see that the elbow of the screeplot is at around 5 principal components. This being said, if we only keep the first 5 components, they would account for a cumulative proportion of variance of only ~63 % of our data (as seen below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e8723-feed-4b46-8fec-ecde2d59df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding how much variation is explained by each component\n",
    "cumulative_sum = np.cumsum(pca.explained_variance_ratio_) \n",
    "print(f\"Proportion of Variance Explained : {np.round(pca.explained_variance_ratio_,2)}\")   \n",
    "print(\"Cumulative Prop. Variance Explained: \", cumulative_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609c531-1f7f-4b7c-9785-574c49cc9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The matrix of variable loadings (i.e., matrix whose columns contain the eigenvectors)\n",
    "# The eigenvectos provide the coefficients for the linear combo\n",
    "rotation = pd.DataFrame(pca.components_, columns = feat_cal.columns[2:]).T \n",
    "rotation.columns = [f\"PC_{i}\" for i in range(1, len(pca.components_) + 1)]\n",
    "rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163718a-2b5a-43ad-951d-74c119f3d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top 5 features that load on the first PC\n",
    "rotation.sort_values(by=['PC_1'], key = abs, ascending = False).head(5)[['PC_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd1ac1-76df-4dc1-bc6a-76c75918ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top 5 features that load on the second PC\n",
    "rotation.sort_values(by=['PC_2'], key = abs, ascending = False).head(5)[['PC_2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd0ef49-a621-42cc-afa3-de544a5639c8",
   "metadata": {},
   "source": [
    "For detailed information on these features and what they represent, the tsfresh documentation can be consulted here: [link](https://tsfresh.readthedocs.io/en/latest/text/list_of_features.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c4ad9-89dd-4aea-9317-17d725b02674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding labels (True: For ECD, False: For unsucessful readings that arent ECD errors) to the scores for visualizations purposes.\n",
    "y = feat_cal[['TestId', 'label']]\n",
    "principalDfLabel = pd.concat([y, principalDf, ], axis = 1)\n",
    "principalDfLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ea649-9e07-466f-8aae-e824cff270bf",
   "metadata": {},
   "source": [
    "#### **Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b92f7d-6384-4c93-a345-e1ca66354ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(principalDfLabel, title = 'Unstandardized Calibration Window').mark_point().encode(\n",
    "    alt.X('PC_1'),\n",
    "    alt.Y('PC_2'),\n",
    "    alt.Color('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23211d45-93f7-4deb-8e47-05d072475c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(principalDfLabel).mark_circle().encode(\n",
    "    alt.X(alt.repeat(\"column\"), type='quantitative'),\n",
    "    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n",
    "    alt.Color('label')\n",
    ").properties(\n",
    "    width=150,\n",
    "    height=150\n",
    ").repeat(\n",
    "    row = [f\"PC_{i}\" for i in range(1, len(pca.components_) + 1)][0:5],\n",
    "    column = [f\"PC_{i}\" for i in range(1, len(pca.components_) + 1)][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b1b10-088c-47d1-b27b-e50d90b5443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D representation\n",
    "plotly.offline.init_notebook_mode()\n",
    "\n",
    "# Changing True/False label to a color because somehow passing the label column to the `color` marker does not work.\n",
    "color = []\n",
    "for i in range(len(principalDfLabel)):\n",
    "    if principalDfLabel['label'][i] == True:\n",
    "        color.append(\"orange\")\n",
    "    else:\n",
    "        color.append(\"blue\")\n",
    "\n",
    "# Configure the trace.\n",
    "trace = go.Scatter3d(\n",
    "    x = principalDfLabel['PC_1'],  \n",
    "    y = principalDfLabel['PC_2'], \n",
    "    z = principalDfLabel['PC_3'],\n",
    "    mode = 'markers',\n",
    "    marker ={\n",
    "        'size': 5,\n",
    "        'opacity': 0.8,\n",
    "        'color':color\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure the layout.\n",
    "layout = go.Layout(\n",
    "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "plot_figure = go.Figure(data = data, layout = layout)\n",
    "\n",
    "# Render the plot.\n",
    "plotly.offline.iplot(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb56ce-81cc-431b-8089-1609ed2a9f08",
   "metadata": {},
   "source": [
    "### **3- CLUSTERING USING THE COMPONENTS FROM PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125402f8-a3fc-4b76-8e99-8be3f4982eb3",
   "metadata": {},
   "source": [
    "#### **3.1) Using Gaussian Mixture Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c084e-3e61-43fd-bf55-b0a8a07fb2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of unique return codes in our data to help us set an initial number of cluster (Here we have 31)\n",
    "# ecd_pred = pd.read_csv(\"../../../../data/RawDataPredictors/New/ECDContact.csv\")\n",
    "# syn_pred = pd.read_csv(\"../../../../data/RawDataPredictors/New/SyntheticECD.csv\") \n",
    "# un_pred = pd.read_csv(\"../../../../data/RawDataPredictors/New/Unsuccessful.csv\")\n",
    "\n",
    "# num_returncodes = len(np.unique(np.concatenate((np.unique(un_pred['ReturnCode']),np.unique(ecd_pred['ReturnCode'])))))\n",
    "# num_returncodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399fe3e2-d51c-4f46-b90e-f4e58fbc124c",
   "metadata": {},
   "source": [
    "In this pipeline we applied Gaussian Mixture Modelling as the first clustering algorithm. Other clustering algorithms such as hierarchical clustering could have been used instead with various types of links (they have been tried, and yielded similar results). The code for hierarchical clustering is provided but commented out for the purpose of this demonstration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba702dd4-0291-4330-96d5-99815d4f084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(5, 5))  \n",
    "# plt.title(\"Dendrograms\")  \n",
    "# dend = shc.dendrogram(shc.linkage(principalDf, method='ward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82916236-8452-4f19-a6d6-ee1bbb42b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier_clustering = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
    "# hier_clustering.fit_predict(principalDf)\n",
    "# y['clusters'] = hier_clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62adbb19-110a-4009-8f94-5645669d5775",
   "metadata": {},
   "source": [
    "Back to Gaussian Mixture Modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9702f8e-e097-460e-8cc0-b8c3786b0f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components = 31)\n",
    "gmm.fit(principalDf)\n",
    "\n",
    "#predictions from gmm\n",
    "clusters = gmm.predict(principalDf)\n",
    "\n",
    "y['clusters'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c91e4b-3fdf-43f6-94cb-b1fd7c2029c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f3b3e-6125-495d-ba31-290413ae5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting an idea of how the clusters are distributed\n",
    "num_clusters = 31\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(f\"The proportion of ECD errors in cluster {i} is: {np.round(len(y[(y['label'] == True) & (y['clusters'] == i)])/len(y[y['clusters'] == i]),4)}\")\n",
    "    print(f\"Out of all the {len(ecd_cal_tot)} ECDs, {len(y[(y['label'] == True) & (y['clusters'] == i)])} are in cluster {i}, which represents {np.round((len(y[(y['label'] == True) & (y['clusters'] == i)])/len(ecd_cal_tot))*100, 2)}% of all the total ECDs\") \n",
    "    print(f\"The total number of readings in cluster {i} is {len(y[y['clusters'] == i])}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585eea9-ef1b-49fc-8710-ba6347a85afd",
   "metadata": {},
   "source": [
    "Ideally, we would want a cluster that contains a high proportion of ECDs (i.e a proportion of 50% would mean that the cluster contains an equal amount of ecd errors and unsuccessful readings) as well as a cluster that contains most of the ECDs. We could then try applying another clustering algorithm only on that cluster in order to further extract the ECDs from the unsuccessful readings.\n",
    "\n",
    "We have decided to take a closer look at the clusters that are made up of at least 20% ECDs and include at least 20% of all the ECDs (i.e 20% of 324 ~ 65 ECDs). These threshold can be changed in the following cell as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32273130-9506-418c-b2f6-61ec126f325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_clusters = []\n",
    "for i in range(num_clusters):\n",
    "    prop_ecd = (len(y[(y['label'] == True) & (y['clusters'] == i)])/len(ecd_cal_tot)) # (number of ecd in cluster i)/(total number of ecd)\n",
    "    prop_ecd_cluster = len(y[(y['label'] == True) & (y['clusters'] == i)])/len(y[y['clusters'] == i]) # (number of ecd in cluster i)/(total number of readings in cluster i)\n",
    "    if (prop_ecd > 0.2) & (prop_ecd_cluster > 0.2):\n",
    "        important_clusters.append(i)\n",
    "important_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1675b-a5d7-4516-a5b4-129900d5b95a",
   "metadata": {},
   "source": [
    "##### **3.1.1) Forming subclusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22358df6-ba12-47a3-91c3-b4a5d9be45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subclusterids = y[y['clusters'].isin(important_clusters)][['TestId', 'label', 'clusters']].reset_index(drop = True)\n",
    "subclusterPC = principalDfLabel[principalDfLabel['TestId'].isin(subclusterids['TestId'])].reset_index(drop = True)\n",
    "\n",
    "# Merging the cluster for visualization purposes\n",
    "subclusterPC = subclusterPC.merge(subclusterids[['TestId', 'clusters']], on = 'TestId')\n",
    "subclusterPC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c765749-353a-492f-a48f-f0e01e6fd738",
   "metadata": {},
   "outputs": [],
   "source": [
    "subclusterY = y[y['TestId'].isin(subclusterids['TestId'])].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb27e4e-b4e7-4b2a-ad6e-8bc9b351ec4c",
   "metadata": {},
   "source": [
    "##### **3.1.2) Visualizing the components only for the subclusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828907a-a354-477e-afd8-f4f1aa8a015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(subclusterPC, title = 'Unstandardized Calibration Window for the sub clusters').mark_point().encode(\n",
    "    alt.X('PC_1'),\n",
    "    alt.Y('PC_2'),\n",
    "    alt.Color('label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87ee70-df5b-4ab5-bffb-ee530d7bf25d",
   "metadata": {},
   "source": [
    "From the above plot, we can easily imagine why it would be hard to cluster out the ECDs (orange) from the unsucessful readings (blue). Lets try anyways using hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bfd35-d0ad-46d9-a6fe-3d3b8d80f1fa",
   "metadata": {},
   "source": [
    "#### **3.2) Subclustering using hierarchical clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6965c7-ece3-47c8-9e5f-ad99b4c68ddd",
   "metadata": {},
   "source": [
    "Once again, any type of clustering could have been used here instead of hierarchical clustering. We have also tried using Gaussian Mixture Modelling, BIRCH, DBSCAN without obtaining significantly better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e65607-be14-4458-9b6c-2bb3433c0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))  \n",
    "plt.title(\"Dendrograms\")  \n",
    "dend = shc.dendrogram(shc.linkage(subclusterPC.drop(['TestId', 'label', 'clusters'], axis = 1), method='ward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78dfbc-0983-4592-97d1-d71274423875",
   "metadata": {},
   "source": [
    "From the dendrogram, we see that 2 clusters would be optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e8be4-70ca-47cf-8ae1-db17a7dd4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_clustering = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
    "hier_clustering.fit_predict(subclusterPC.drop(['TestId', 'label', 'clusters'], axis = 1))\n",
    "\n",
    "subclusterY['sub_clusters'] = hier_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b5fd39-1a96-45ca-8881-b70d0f34cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subclusterY[subclusterY['label'] == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965fa56e-3ee4-4336-85ea-45f56b1c0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(f\"The proportion of ECD errors in cluster {i} is: {np.round(len(subclusterY[(subclusterY['label'] == True) & (subclusterY['sub_clusters'] == i)])/len(subclusterY[subclusterY['sub_clusters'] == i]),4)}\")\n",
    "    print(f\"Out of all the {len(subclusterY[subclusterY['label'] == True])} ECDs in cluster(s) {important_clusters}, {len(subclusterY[(subclusterY['label'] == True) & (subclusterY['sub_clusters'] == i)])} are in cluster {i}, which represents {np.round((len(subclusterY[(subclusterY['label'] == True) & (subclusterY['sub_clusters'] == i)])/len(subclusterY[subclusterY['label'] == True]))*100, 2)}%\") \n",
    "    print(f\"The total number of readings in subcluster {i} is {len(subclusterY[subclusterY['sub_clusters'] == i])}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528d2f8-1991-4779-8545-eb5f5445bc58",
   "metadata": {},
   "source": [
    "As we can see from the output above, the size of the 2 clusters formed are very similar to one another and so is the proportion of ECDs in each cluster. We were thus not able to successfully extract the ECDs from this pipeline.\n",
    "\n",
    "Further investigation using other clustering algorithms and/or different parameters and/or applied to a different time window could yield better (or worse) results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52590140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
