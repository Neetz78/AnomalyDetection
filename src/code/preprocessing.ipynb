{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d4de93-a530-4ffa-b261-c55634241088",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Preprocessing Notebook Description**\n",
    "\n",
    "Description: this notebook contains functions that window, normalize, and smooth the data. At the bottom a sequence of steps using them to preprocess the data is demonstrated. First the time series are subset from -30 seconds before sample detect time to 40 seconds after sample detect time. If this full time range is not present in a reading, then that reading is dropped. Next, the time series are normalized to scale their values between 0 and 1. Finally, a convolution smoother with a bartlett window of length 50 is applied to smooth the time series. The time series are then written into fresh csv files. \n",
    "\n",
    "## **Imports**\n",
    "Here we have all of the import statements needed for the packages used for preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9f0f2-c7f6-4455-8eb4-c64d25fdfbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from tsmoothie.smoother import *\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136f9ab-ed26-4bbc-963b-4b3674109477",
   "metadata": {},
   "source": [
    "## **Load Raw Original Data**\n",
    "\n",
    "Here all of the original raw time series and predictor files are read into pandas data frames. \n",
    "\n",
    "**You will need to change the paths to match where you have these files stored**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b15cc-7cb6-42e6-9abe-b72ecc2bc5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the original time series data files. \n",
    "ecd_ts = pd.read_csv('../Data/RawData/TimeSeries/PC_TS.csv')\n",
    "un_ts = pd.read_csv('../Data/RawData/TimeSeries/US_TS.csv')\n",
    "syn_ts = pd.read_csv('../Data/RawData/TimeSeries/PC_TS_Synthetic.csv')\n",
    "con_ts =  pd.read_csv('../Data/RawData/TimeSeries/PC_TS_Contaminated.csv')\n",
    "\n",
    "# Import the original Predictors files\n",
    "syn_pred = pd.read_csv('../Data/RawData/Predictors/SyntheticPC.csv')\n",
    "con_pred =  pd.read_csv('../Data/RawData/Predictors/PCAggContaminated.csv')\n",
    "rem_pred = pd.read_csv('../Data/RawData/Predictors/RawAggData2021-2022.csv')\n",
    "\n",
    "\n",
    "# Import the original predictor ID files\n",
    "ecd_id = pd.read_csv('../Data/RawData/Predictors/TestID_PC.txt', sep = ',', header = None, squeeze = True).transpose()\n",
    "un_id = pd.read_csv('../Data/RawData/Predictors/TestID_Unsuccessful.txt', sep = ',', header = None, squeeze = True).transpose()\n",
    "\n",
    "# Drop the duplicated predictors (125 of the records were duplicated but with different data formats, all other info is the same)\n",
    "dups = rem_pred['TestID'].duplicated()\n",
    "#removed the duplicate 125 IDs.\n",
    "rem_pred = rem_pred[~dups]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d325b-0744-4bc9-8173-57188db12cca",
   "metadata": {},
   "source": [
    "## **Separate the predictors out for the different categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f907d6bf-5bcf-48db-b4db-5061ebe86aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the wild ECD and unsuccessful predictors. \n",
    "ecd_pred = rem_pred[rem_pred['TestID'].isin(ecd_id[0])]\n",
    "un_pred = rem_pred[rem_pred['TestID'].isin(un_id[0])]\n",
    "\n",
    "# Save the separated predictor files for future use. \n",
    "ecd_pred.to_csv('../Data/PreprocessedData/Predictors/ECD.csv', index = False)\n",
    "un_pred.to_csv('../Data/PreprocessedData/Predictors/Unsuccessful.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf03b7-fbd5-4561-9351-a81fb0c05a5d",
   "metadata": {},
   "source": [
    "## **Windowing Functions**\n",
    "### Function to window and zero the original data\n",
    "This function takes the start time with respect to sample detect time (-30 means 30 seconds before sample detect time), the end time (40 would mean 40 seconds after sample detect time), a data frame with the time series where each row is a time series and each column is a time point. It is assumed that the sampling rate is 5 Hz, and that the data frame contains a 'TestId'column. It is also assumed that the time-series that are passed in have not already been altered, so the first time point is 0.2 (corresponding to the first sample). Finally, the dataframe with the aggregate predictors for the time series must also be passed in so that sample detect time can be computed. The returned dataframe has the time series zeroed with respect to sample detect time and subset to the times between start and end. The data frame is also reindexed so that the column names are the time in seconds with respect to sample detect time (eg. '-30.0', '-28.8', ... '0.0', ... '40.0'). \n",
    "\n",
    "Note that this function also drops any readings with a sample detect time at 0. When we looked at the data, all of these readings were unsuccessful with the return code `CannotCalculate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21070cea-4afa-48ab-b631-174fe12a6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the times series based on start and end times relative to sample detect time. \n",
    "# start - seconds to start from relative to sample detect (eg. -5 would start 5 seconds before, 5 would start 5 seconds after)\n",
    "# end - seconds to end from relative to sample detect. \n",
    "# ts - data frame where each row is a time series\n",
    "# pred - data frame where each row is a reading, has the sample detect time column. \n",
    "def window_and_zero(start, end, ts, pred):\n",
    "    #Remove records with sample detect time at 0. \n",
    "    ts = ts[ts['TestId'].isin(pred['TestID'][pred['SampleDetectTime']!=0])]\n",
    "    \n",
    "    # Get the list of IDs for both data sources. \n",
    "    ts_ids = ts['TestId']\n",
    "    pred_ids = pred['TestID']\n",
    "    \n",
    "    #Drop ids from the time series. \n",
    "    ts = ts.drop('TestId', axis = 1)\n",
    "    \n",
    "    # Convert sample detect time to samples from second\n",
    "    sample_detect = (pred['SampleDetectTime']/0.2).astype(int)\n",
    "    \n",
    "    # Make a data frame with test ids, and start and end indices to window based on. \n",
    "    index = pd.concat([pred_ids, int(start/0.2)+sample_detect, int(end/0.2) +sample_detect], axis = 1)\n",
    "    index.columns = [\"TestId\", \"Start\", \"End\"]\n",
    "    # Merge start and end indices into time series data frame based on ids. \n",
    "    ts['TestId'] = ts_ids\n",
    "    ts = ts.merge(index, how = 'left', on = 'TestId')\n",
    "    # Save the order of ids for later use. \n",
    "    ids = ts['TestId']\n",
    "    \n",
    "    #Subset each time series based on it's start and end indices. Uses a list comprehension since the indices will be different for each time series. \n",
    "    subsets = [ts.iloc[row, ts['Start'][row]:ts['End'][row]].reset_index(drop = True) for row in range(len(ts))]\n",
    "    \n",
    "    #Now that all time series are the same length, and zeroed w.r.t sample detect, convert back to a dataframe. \n",
    "    subsets = pd.DataFrame(subsets)\n",
    "    \n",
    "    #Rename the columns to match the zeroed time stamps. \n",
    "    subsets.columns =  [str(round(m,1)) for m in np.arange(start,end, 0.2)]\n",
    "    \n",
    "    #Return a dataframe with test ids reatttached. \n",
    "    return pd.concat([pd.DataFrame(ids),subsets], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d81481-e456-46a6-aee1-47f9c4d942f7",
   "metadata": {},
   "source": [
    "### Function to window zeroed data \n",
    "\n",
    "This function can be used to subset the data frames that are output by the window_and_zero function. It is useful if you want to find something like the calibration/post/sample windows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ab225-45ee-450d-aefb-d68387bbdffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the zeroed data. Useful if you want to pull out calibration/sample/post windows. \n",
    "# start - the start time you want to subset w.r.t sample detect time, in seconds. Must be evenly divisuble by 0.2 since that's the sampling rate of the data. \n",
    "#\n",
    "# end - the end time you want to subset w.r.t sample detect time, in seconds. It must be evenly divisuble by 0.2 since that's the sampling rate of the data. \n",
    "#\n",
    "# zeroed_ts - a dataframe ouput by the window_and_zero() function. \n",
    "\n",
    "def window_zeroed(start, end, zeroed_ts):\n",
    "    # Make sure there's only one decimal place. Also that they are in string format. \n",
    "    start = str(round(float(start), 1))\n",
    "    end = str(round(float(end), 1))\n",
    "    \n",
    "    #Subest and return the data frame. \n",
    "    res = zeroed_ts.loc[:,start:end]\n",
    "    res = pd.concat([zeroed_ts['TestId'], res], axis = 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec68e31-279c-4bb3-b350-ecadd68d3a23",
   "metadata": {},
   "source": [
    "## **Scaling Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988054a-1312-45b1-99f6-c682c0fea365",
   "metadata": {},
   "source": [
    "### Function to normalize\n",
    "\n",
    "This function will normalize the data. This means that each time series will be scaled so that it's values are scaled between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00752a-ff72-4d26-8ec1-2acd6fa16971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scales the time series in a dataframe so that their values are between 0 and 1. \n",
    "# ts_df: A frame where each row is a time series and each column is a time point, with an additional `TestId` column. \n",
    "# Returns a data frame where each row is a time series with values scale between 0 and 1. \n",
    "def normalize(ts_df):\n",
    "    ids = ts_df['TestId'] #Save the ids for later use. \n",
    "    subset = ts_df.drop('TestId', axis = 1)\n",
    "    m = subset.min(axis = 1) #New data frame with the min value for each time series.\n",
    "    r = subset.max(axis = 1) - m #New data frame with range value for each time series. \n",
    "\n",
    "    #When r==0, it means that min = max and thus dividing by 0 (creating nans). Instead, set m to 0 and r to 1 so that readings that are completely flat are unchanged\n",
    "    m[r==0] = 0\n",
    "    r[r==0] = 1\n",
    "\n",
    "    #Subtract the mean of the time series from each time point. \n",
    "    st1 = subset.sub(m, axis = 0) \n",
    "    #Divide each time point by the standard deviation of the time series. \n",
    "    norm = st1.div(r, axis = 0)\n",
    "\n",
    "    #Return the standardized time series with test ids back in the first column. \n",
    "    return pd.concat([ids, norm], axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb8e8b0-47ae-4a4c-8379-51fe2c0b6994",
   "metadata": {},
   "source": [
    "### Function to standardize\n",
    "\n",
    "This function will standardize the data. This means that each time series will be scaled to have mean 0 and standard deviation 1. One caveat is that time series with a standard deviation of 0 will be left with a standard deviation of 1 to avoid division by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26509b35-80a4-41b3-be28-863c6f841fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all the time series in a dataframe (transform each time series to have mean 0 and standard deviation 1).\n",
    "# ts_df - data frame where each row is a time series, has columns for time points, and one column with TestId.\n",
    "# Returns a new data frame where each time series is standardized to have mean 0 and standard deviation 1. \n",
    "def standardize(ts_df):\n",
    "    ids = ts_df['TestId']\n",
    "    subset = ts_df.drop('TestId', axis = 1)\n",
    "    m = subset.mean(axis = 1) #New data frame with the meam value for each time series.\n",
    "    r = subset.std(axis = 1) #New data frame with standard deviation for each time series. \n",
    "\n",
    "    #Set r to 1 when the standard deviation is 0 so that readings that are completely flat don't becom nans when divided by 0. \n",
    "    r[r==0] = 1\n",
    "\n",
    "    #Subtract the mean of the time series from each time point. \n",
    "    st1 = subset.sub(m, axis = 0) \n",
    "    #Divide each time point by the standard deviation of the time series. \n",
    "    norm = st1.div(r, axis = 0)\n",
    "\n",
    "    #Return the standardized time series with test ids back in the first column. \n",
    "    return pd.concat([ids, norm], axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e16d5-06ea-4779-9988-eaabbd970a8b",
   "metadata": {},
   "source": [
    "## **Noise Reduction Functions**\n",
    "\n",
    "### Function to smooth with a moving average \n",
    "\n",
    "Uses a convolution smoother with a uniform rectangular kernal to smooth using the moving average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e15bc-4e6e-42d8-b509-341b85f7ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth all the waveforms in a dataframe using a moving average via the convolution smoother from tsmoothie along with a \n",
    "# rectangular window. \n",
    "# ts - data frame where each row is a time series, has columns for time points, and one column with TestId. Must not contain any nas. \n",
    "# wl - the length of the window used to convolve.\n",
    "# Start - the number of seconds before sample detect where the readings start.\n",
    "# Stop - the number of seconds after sample detect where the readings end.\n",
    "def moving_average_smooth(ts, wl = 30, start = -30, end = 40):\n",
    "    #Save ids\n",
    "    ids = ts['TestId'].reset_index(drop = True)\n",
    "    # operate smoothing\n",
    "    smoother = ConvolutionSmoother(window_len=wl, window_type='ones')\n",
    "    smoother.smooth(ts.drop('TestId', axis = 1))\n",
    "    smooth = pd.DataFrame(smoother.smooth_data)\n",
    "    smooth.columns = [str(round(m,1)) for m in np.arange(start,end, 0.2)]\n",
    "    smooth['TestId'] = ids\n",
    "    return smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e361ce43-61cb-4865-b766-b760059bd6ab",
   "metadata": {},
   "source": [
    "### Function to smooth using a convolution smoother with a bartlett window\n",
    "\n",
    "Using the bartlett window should lead to less distorion on the edges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de00811-6c3f-4d34-89a3-b043bccc8168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth all the waveforms in a dataframe using the convolution smoother from tsmoothie along with a \n",
    "# Bartlett window. \n",
    "# ts - data frame where each row is a time series, has columns for time points, and one column with TestId. Must not contain any nas. \n",
    "# wl - the length of the window used to convolve.\n",
    "# Start - the number of seconds before sample detect where the readings start.\n",
    "# Stop - the number of seconds after sample detect where the readings end.\n",
    "def bartlett_convolve_smooth(ts, wl = 50, start = -30, end = 40):\n",
    "    #Save ids\n",
    "    ids = ts['TestId'].reset_index(drop = True)\n",
    "    # operate smoothing\n",
    "    smoother = ConvolutionSmoother(window_len=50, window_type='bartlett')\n",
    "    smoother.smooth(ts.drop('TestId', axis = 1))\n",
    "    smooth = pd.DataFrame(smoother.smooth_data)\n",
    "    smooth.columns = [str(round(m,1)) for m in np.arange(start,end, 0.2)]\n",
    "    smooth['TestId'] = ids\n",
    "    return smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf303b6-e10b-45f4-b9b8-73cc38d2e4be",
   "metadata": {},
   "source": [
    "### Function to smooth using a Savitzy-Golay filter\n",
    "\n",
    "Works similarly to a moving average but instead of taking the average of the points to adjust their values, it fits a polynomial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773ff0b-38a3-4c9d-96b4-31de5772bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savgol_smooth(ts, wl = 51, polyorder = 2, start = -30, end = 40):\n",
    "    #Save ids\n",
    "    ids = ts['TestId'].reset_index(drop = True)\n",
    "    # operate smoothing\n",
    "    smoother.smooth(ts.drop('TestId', axis = 1))\n",
    "    smooth = savgol_filter(ts.drop('TestId', axis = 1).to_numpy(),  wl, polyorder = polyorder, deriv=0, axis = 1)\n",
    "    smooth = pd.DataFrame(smooth)\n",
    "    smooth.columns = [str(round(m,1)) for m in np.arange(start,end, 0.2)]\n",
    "    smooth['TestId'] = ids\n",
    "    return smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed118245-fc94-4409-89f6-9c4120798b3f",
   "metadata": {},
   "source": [
    "## **Diagnostic Functions**\n",
    "\n",
    "### Function to plot 5 random smoothed waveforms on top of the original ones, and the associated periodogram below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d85a64-674f-4294-989d-1684d916f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_smooth(unfiltered_ts, filtered_ts):\n",
    "    # get 5 random test ids\n",
    "    ides = list(unfiltered_ts['TestId'].sample(n=5))\n",
    "    \n",
    "    # get first and last timestamp to label the x-axis in plot\n",
    "    start = unfiltered_ts.columns[~unfiltered_ts.columns.str.isalpha()][0]\n",
    "    end = unfiltered_ts.columns[~unfiltered_ts.columns.str.isalpha()][-1]\n",
    "    \n",
    "    ## Plot the overlayed waveforms\n",
    "    \n",
    "    # define subplot grid\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(30, 5))\n",
    "\n",
    "    # loop through tickers and axes\n",
    "    for ide, ax in zip(ides, axs.ravel()):\n",
    "        # plot the filtered and unfiltered versions for the test id. \n",
    "        ax.plot(unfiltered_ts[unfiltered_ts['TestId'] == ide].drop('TestId', axis = 1).transpose(), c = 'b', alpha = 0.3, label = 'unfiltered')\n",
    "        ax.plot(filtered_ts[filtered_ts['TestId'] == ide].drop('TestId', axis = 1).transpose(), c = 'b', label = 'filtered')\n",
    "        ax.xaxis.set_ticks([start, '-0.0', end])\n",
    "        ax.set_xlabel('time')\n",
    "        ax.set_ylabel('signal')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    ## Plot the overlayed periodograms\n",
    "    \n",
    "    # define subplot grid\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(30, 5))\n",
    "\n",
    "    # loop through tickers and axes\n",
    "    for ide, ax in zip(ides, axs.ravel()):\n",
    "        # plot the filtered and unfiltered versions for the test id. \n",
    "        f, ps = signal.periodogram(unfiltered_ts[unfiltered_ts['TestId'] == ide].drop('TestId', axis = 1), 5)\n",
    "        f1, ps1 = signal.periodogram(filtered_ts[filtered_ts['TestId'] == ide].drop('TestId', axis = 1), 5)\n",
    "        \n",
    "        ax.plot(f, ps[0], c = 'b', alpha = 0.3, label = 'unfiltered')\n",
    "        ax.plot(f1, ps1[0], c = 'b', label = 'filtered')\n",
    "        ax.set_xlabel('frequency (Hz)')\n",
    "        ax.set_ylabel('power')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491da960-78ea-47d1-9d36-46fb01f55a28",
   "metadata": {},
   "source": [
    "## **Demo of a preprocessing pipeline**\n",
    "\n",
    "In this preprocessing pipeline, first the time series are subset from -30 seconds before sample detect time to 40 seconds after sample detect time. If this full time range is not present in a reading, then that reading is dropped. Next, the time series are normalized to scale their values between 0 and 1. Finally, convolution smoother with a bartlett window of length 50 is applied to smooth the time series. The time series are then written into fresh csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ccf2b-8333-45f4-ac48-bf0cdf8f1c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining 'non wet-up' as 30 seconds before to 40 seconds after sample detect\n",
    "start = -30\n",
    "end = 40\n",
    "\n",
    "# Subset to -30 seconds before sample detect to 40 seconds after, this will get rid of wetup and make sure everything is aligned. \n",
    "ecd_w = window_and_zero(start, end, ecd_ts, ecd_pred)\n",
    "syn_w = window_and_zero(start, end, syn_ts, syn_pred)\n",
    "con_w = window_and_zero(start, end, con_ts, con_pred)\n",
    "un_w = window_and_zero(start, end, un_ts, un_pred)\n",
    "\n",
    "# Normalize to scale all the waveforms between 0 and 1. \n",
    "ecd_norm = normalize(ecd_w).dropna()\n",
    "syn_norm = normalize(syn_w).dropna()\n",
    "con_norm = normalize(con_w).dropna()\n",
    "un_norm = normalize(un_w).dropna()\n",
    "\n",
    "#Smooth with a Bartlett window convolution\n",
    "ecd_smooth = bartlett_convolve_smooth(ecd_norm)\n",
    "un_smooth = bartlett_convolve_smooth(un_norm)\n",
    "con_smooth = bartlett_convolve_smooth(con_norm)\n",
    "syn_smooth = bartlett_convolve_smooth(syn_norm)\n",
    "\n",
    "#Get rid of the weird outliers for now. \n",
    "ecd_smooth = ecd_smooth[~ecd_smooth['TestId'].isin([9610647, 9610462])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ba76e-5b1c-4ec3-8020-fa58fe4fbe5e",
   "metadata": {},
   "source": [
    "### Plot some of theECD errors after filtering on top of the normalized ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d88f71-5acd-41d9-8c4a-4b322e8a0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_smooth(ecd_norm, ecd_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88789e33-230d-41b1-904c-2190b481d3bf",
   "metadata": {},
   "source": [
    "### Save the processed waveforms to new files\n",
    "\n",
    "**You will need to make a folder for them or change the path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a574f6-244b-4b40-9745-e270cf8979de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecd_smooth.to_csv('../Data/PreprocessedData/TimeSeries/ecd_smooth.csv', index = False)\n",
    "syn_smooth.to_csv('../Data/PreprocessedData/TimeSeries/syn_smooth.csv', index = False)\n",
    "con_smooth.to_csv('../Data/PreprocessedData/TimeSeries/cont_smooth.csv', index = False)\n",
    "un_smooth.to_csv('../Data/PreprocessedData/TimeSeries/un_smooth.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
