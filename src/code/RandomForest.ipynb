{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70b7b64",
   "metadata": {},
   "source": [
    "## **Random Forest For Feature Extraction from Predictor file**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a34eca",
   "metadata": {},
   "source": [
    "Random Forest is a supervised model that implements both decision trees and bagging method. In this notebook we used the function (feature_importances_) present in the random forest classifier package [(from the SciKit Learn Package)](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html) to obtain the features that were most important when seperating ecd contact readings from unsuccessful readings. To get these estimates for variable importance we used the labels for unsucecssful and ecd contact errors that we had access to. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ce46d",
   "metadata": {},
   "source": [
    "### Step 1 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ddd89",
   "metadata": {},
   "source": [
    "- Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9536be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing all required libraries \n",
    "\n",
    "import pandas as pd                                    ## Library used for Dataframe manipulation                     \n",
    "import numpy as np                                     ## Library for array manipulation\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder         ## Library to encode all categorical variables\n",
    "from sklearn.model_selection import train_test_split   ## Library for splitting the data into train and test sets\n",
    "from sklearn.ensemble import RandomForestClassifier    ## Random Forest Classifier library \n",
    "from sklearn.metrics import precision_score            ## Metric used to measure the results obtained from Random Forest Classifier\n",
    "from sklearn import model_selection                    ## Library for model selection \n",
    "from sklearn import metrics                            ## Library for metrics to define the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2a136",
   "metadata": {},
   "source": [
    "### Step 2 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc678d",
   "metadata": {},
   "source": [
    "- Read the predictor file and perfomring EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the aggregate predictor files\n",
    "un_pred =  pd.read_csv('../Data/PreprocessedData/Predictors/Unsuccessful.csv')\n",
    "ecd_pred =  pd.read_csv('../Data/PreprocessedData/Predictors/ecdContact.csv')\n",
    "syn_pred =  pd.read_csv('../Data/RawData/Predictors/SyntheticECD.csv')\n",
    "con_pred =  pd.read_csv('../Data/RawData/Predictors/ECDAggContaminated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bef801",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add labels for predictor data\n",
    "\n",
    "un_pred['Label'] = \"Unsuccessful\"\n",
    "ecd_pred['Label'] = \"ecdcontacts\"\n",
    "syn_pred['Label'] = \"Synecd\"\n",
    "con_pred['Label'] = \"Conecd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6860723",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge the predictor files and rename the TestID column to match with timeseries file.\n",
    "\n",
    "preds = pd.concat([un_pred, ecd_pred, syn_pred, con_pred])\n",
    "preds = preds.rename({'TestID':'TestId'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39242347",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reset the index. \n",
    "preds = preds.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8445f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store the TestIds and Label in a variable ids and labels\n",
    "ids, labels = preds[['TestId','Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if any column has all values as zero for all records.\n",
    "(preds == 0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520eb14",
   "metadata": {},
   "source": [
    "#### Other predictor file columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439b5b9",
   "metadata": {},
   "source": [
    "- descriptions dropped for confidentiality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop all of the above mentioned columns.\n",
    "\n",
    "preds.drop(['list of aggregate predictors'],axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper function to find the correlation matrix of a given dataset and drop the columns have a positive correlation above 0.95.\n",
    "    Args:\n",
    "        data : dataframe for which correlation needs to be found.\n",
    "        percent : the percent of correlation between the column which crosses over and needs to be dropped.\n",
    "    Returns : dataframe with correlated columns dropped\n",
    "    \"\"\"\n",
    "def frame(data,percent):\n",
    " \n",
    "    ## Calculate the pairwise correlation matrix of all the column present in the dataframe\n",
    "    cor_matrix = data.corr()                               \n",
    "    \n",
    "    ## Calculate only the upper triangle of the correlation matrix as it is symmetric and storing it in an variable called upper_triangle \n",
    "    upper_triangle = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\n",
    "    \n",
    "    ## Calculate all the columns whose positive corelation is greater is 0.95 and storing them in a variable to_drop\n",
    "    ## Change 0.95 to +/- to obtain positive or neagative correlation matrix of the columns.\n",
    "    ## We have chosen 0.95 here it can be set to account for value to your choosing.\n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > percent)]\n",
    "    \n",
    "    ## Drop the columns stored in to_drop variable from the dataframe and returning the dataframe.\n",
    "    data.drop(to_drop,axis=1, inplace= True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call the function frame to drop the columns with desired correlation which is the second parameter passed.\n",
    "\n",
    "preds = frame(preds, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550206d",
   "metadata": {},
   "source": [
    "### Step 3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bfefb4",
   "metadata": {},
   "source": [
    "- Separate the predictor and labels\n",
    "- Encode the categorical variables present in the predictor file\n",
    "    - AggPred X and AggPredY\n",
    "- Split the data into training  and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694dd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate the predictors and labels into varibles.\n",
    "\n",
    "X1 = preds.iloc[:,:-1]\n",
    "Y1 = preds.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932dc0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert categorical variables to integers.\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "X1['AggPredX'] = le.fit_transform(X1['AggPredX'])\n",
    "X1['AggPredY'] = le.fit_transform(X1['AggPredY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d82aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into training and testing dataset:\n",
    "## This is necessary to prevent overfitting.\n",
    "## Parameters include:\n",
    "    ## X_train,X_test,Y_train & Y_test = data is split in to train and test\n",
    "    ## test_size = 80-20 ratio is chosen here hence 0.2 is the size of the test set\n",
    "    ## shuffle = (True) to shuffle the data, so a diverse set is chosen instead of the same labeled ones.\n",
    "    \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X1, Y1, test_size=0.2, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e9227",
   "metadata": {},
   "source": [
    "### Step 4 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1cb54c",
   "metadata": {},
   "source": [
    "Iniatiate the Random Forest Classifier model to train the model and use the feature_importance function to obtain the scores of all the columns to choose the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9133d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = RandomForestClassifier(n_estimators=100)          ## Build the Random Forest classifier model with 100 trees\n",
    "mod1.fit(X_train,Y_train)                                ## Fit the model on tranining data (X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef984fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the function feature_importance_ on the ranodm forest model built to obtain the scores assigned for each predictor.\n",
    "\n",
    "feature_scores = pd.Series(mod1.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "feature_scores  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b654a5",
   "metadata": {},
   "source": [
    "### Results :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d087d",
   "metadata": {},
   "source": [
    "AggPredC and AggPredD seems to have the higher scores and as we proceed further down we see the scores decreasing for other predictors\n",
    "\n",
    "We will consider these as our predictors to be combined with the PCA componenets obtained from the encoder to form our final dataframe we use for clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
