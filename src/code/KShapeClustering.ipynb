{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95594892-5ce1-42c6-a9e3-a48ccb72b76b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **KShape Clustering Notebook Description**\n",
    "\n",
    "In this notebook, we show how we used the KShape algorithm to cluster the readings. We used this algorithm with various data as our input and concluded that the best outcome resulted from us using:\n",
    "* **The timeseries readings windowed from -30 to 40 with respect to sample detect time**\n",
    "* **Normalizing these readings so that they are all between 0 and 1**\n",
    "* **Concatenating the 10 most important predictors obtained from the variable importance plot from the RandomForest algorithm**\n",
    "* **Standardzing each row(reading) to have a mean of 0 and a standard deviation of 1**\n",
    "* **Clustering with K = 30**\n",
    "\n",
    "Here is a list of other pipelines that were tried using KShape. They did not yield successful results :\n",
    "1. *Using only the timeseries readings*\n",
    "    * Window timeseries readings from -30 to 40 w.r.t sample detect time\n",
    "    * Normalize these readings so that they are all between 0 and 1\n",
    "    * Standardize each reading to have a mean of 0 and a s.d of 1\n",
    "    * Cluster with K = [20, 30]\n",
    "\n",
    "2. *Using the timeseres readings + predictors with K = 20*\n",
    "    * The timeseries readings windowed from -30 to 40 with respect to sample detect time\n",
    "    * Normalizing these readings so that they are all between 0 and 1\n",
    "    * Concatenating the 10 most important predictors obtained from the variable importance plot from the RandomForest algorithm\n",
    "    * Standardzing each row(reading) to have a mean of 0 and a standard deviation of 1\n",
    "    * Clustering with K = 20\n",
    "    \n",
    "3. *Not normalizing the timeseries data*\n",
    "    * The timeseries readings windowed from -30 to 40 with respect to sample detect time\n",
    "    * Concatenating the 10 most important predictors obtained from the variable importance plot from the RandomForest algorithm\n",
    "    * Standardzing each row(reading) to have a mean of 0 and a standard deviation of 1\n",
    "    * Clustering with K = 30\n",
    "\n",
    "The clustering for this pipeline took much longer to run as it was not converging. The clusters obtained do not represent distinguishable shapes. After reading the research paper on [KShape](http://www1.cs.columbia.edu/~jopa/Papers/PaparrizosSIGMOD2015.pdf), we understood that the distance measure that is used (cross-correlation) requires for the readings to be contained \"within a specified range [...] in order to meaningfully compare such sequences.\" \n",
    "\n",
    "4. *Normalizing the timeseries data, but not standardizing it*\n",
    "    * The timeseries readings windowed from -30 to 40 with respect to sample detect time\n",
    "    * Normalizing these readings so that they are all between 0 and 1\n",
    "    * Concatenating the 10 most important predictors obtained from the variable importance plot from the RandomForest algorithm\n",
    "    * Clustering with K = 30\n",
    "\n",
    "We found that without standardizing the rows, the clusters obtained, once again, did not represent distinguishable shapes. After reading the research paper on [KShape](http://www1.cs.columbia.edu/~jopa/Papers/PaparrizosSIGMOD2015.pdf), we understood that to achieve scale invariance, we had to transform \"each sequence [...] so that its mean µ is zero and its standard deviation σ is one\". In the context of this problem (clustering the unsuccessful readings), scaling the readings to have mean 0 and s.d 1 is justifiable as it removes the possible impact that using different fluid types (for example), can have on the readings. \n",
    "    \n",
    "5. *Using PCA to reduce the dimension of the timeseries data*\n",
    "    * The timeseries readings windowed from -30 to 40 with respect to sample detect time\n",
    "    * Normalizing these readings so that they are all between 0 and 1\n",
    "    * Standardize each column (representing time points) to have mean 0 and standard deviation 1\n",
    "    * Applying PCA for dimension reduction\n",
    "    * The 3 first components accounted for 95 % of the cumulative variance\n",
    "    * Concatenating the 10 most important predictors obtained from the variable importance plot from the RandomForest algorithm\n",
    "    * Standardzing each column (3 first PC and 10 predictors) to have a mean of 0 and a standard deviation of 1\n",
    "    * Clustering with K = 30\n",
    "\n",
    "This yielded bad results. From our understanding, it might not make sense to standardize each column from the timeseries data. By doing so, we are assuming that there is equal variance at each time point across the different readings. This is probably not the case (less variance during the calibration window than during the sample window for example) and so by standardizing we are loosing this information that might be useful for clustering. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf95ee4-d822-4be8-90a7-529d449991a2",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b38f5-c4e3-4abd-8a44-b81006f7805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from tslearn.clustering import KShape\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "# Import the in house diagnostic function\n",
    "from diagnostics import *\n",
    "\n",
    "# Removes the warning when adding a column to df\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895a12c-8284-4baa-aa69-6e2d1f1f76a6",
   "metadata": {},
   "source": [
    "## **Read in the preprocessed data that is to be clustered**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde2ce0-d8c9-43c6-a01c-c5ccfc6122ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the preprocessed time series. \n",
    "# Importing the timeseries data\n",
    "# Read in all the preprocessed time series. \n",
    "ecd_ts = pd.read_csv('../Data/PreprocessedData/TimeSeries/ecd_smooth.csv')\n",
    "syn_ts= pd.read_csv('../Data/PreprocessedData/TimeSeries/syn_smooth.csv')\n",
    "cont_ts= pd.read_csv('../Data/PreprocessedData/TimeSeries/cont_smooth.csv')\n",
    "un_ts= pd.read_csv('../Data/PreprocessedData/TimeSeries/un_smooth.csv')\n",
    "\n",
    "\n",
    "# Make a new data frame with all the timeseries readings (unsuccessful + ECDs)\n",
    "ts = pd.concat([un_ts, ecd_ts, syn_ts, cont_ts, un_ts])\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "# Read in the aggregate predictor files\n",
    "un_pred =  pd.read_csv('../Data/PreprocessedData/Predictors/Unsuccessful.csv')\n",
    "ecd_pred =  pd.read_csv('../Data/PreprocessedData/Predictors/ECD.csv')\n",
    "syn_pred =  pd.read_csv('../Data/RawData/Predictors/SyntheticPC.csv')\n",
    "con_pred =  pd.read_csv('../Data/RawData/Predictors/PCAggContaminated.csv')\n",
    "\n",
    "# Add labels\n",
    "ecd_pred['Label'] = 'wild' # ECDs in the 'wild'\n",
    "syn_pred['Label'] = 'syn'  # Synthetic ECDs\n",
    "con_pred['Label'] = 'con'  # Contaminated ECDs\n",
    "un_pred['Label'] = 'un'    # Unsuccessful readings\n",
    "\n",
    "# Make a new data frame with all the predictor readings (unsuccessful + ECDs)\n",
    "pred = pd.concat([ecd_pred, syn_pred, con_pred, un_pred])\n",
    "pred = pred.rename({'TestID' : 'TestId'}, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a17e360-e0cb-40c5-8e1c-59ca020a5bc7",
   "metadata": {},
   "source": [
    "After obtaining a promising cluster (using KMeans) that contained lots of ECDs, we asked Olivia to verify if the unsuccessful readings that were in that same cluster were ECDs. It turned out that 35 out of the 50 actually were. The other were linked to errors for another analyte (sample bubble or ECD). Thus, we changed the label of these unsuccessful readings to `modified_ecds` instead of `un`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62618c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the label from unsuccessful to 'modified_ecd' for the TestIds we were confirmed by Olivia were actually ECD errors\n",
    "un_ids_checked = pd.read_csv('../Data/RawData/un_ids_checked.csv')\n",
    "pred = pred.merge(un_ids_checked, on = 'TestId', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1091583f-055c-41aa-a92f-fb8e9e78f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(len(pred)):\n",
    "    if (pred['Label'][row] == 'un') & (pred['ECD'][row] == 'Yes'):\n",
    "        pred['Label'][row] = 'modified_ecd'\n",
    "\n",
    "pred = pred.drop(columns = 'ECD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf81b1-99c2-4d9e-b5e0-644f8eed93cf",
   "metadata": {},
   "source": [
    "## **Select a subset of the predictors and concatenate them to our timeseries data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b02e1-3e9b-45ad-aa99-6db07ee25cbc",
   "metadata": {},
   "source": [
    "The predictors that had the highest score when calculating the variable importance after fitting a RandomForest Classifier are used here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec0c9e-be39-4601-8dcb-e581e31cdfc3",
   "metadata": {},
   "source": [
    "**Using different sets of predictors gives us pretty different clusters. See the 2 sets tried below and the resulting distribution of their \"ECD clusters\"**\n",
    "\n",
    "**Uncomment the *pred_subset* you wish to use**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f510b3e-776c-4301-8fa6-4bd4e83fb68e",
   "metadata": {},
   "source": [
    "Using the predictors below, we obtain **one** cluster with most of the ECDs. Here is the distribution of this cluster:\n",
    "* 343 ECDs\n",
    "* 376 unsuccessful readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d2068-917f-4438-941b-f2eaf48cd0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same predictors as for the autoencoder\n",
    "pred_subset = pred[['list of 22 predictors']]\n",
    "pred_subset = pred_subset.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9153298-e589-493d-a99b-575db14e8cb8",
   "metadata": {},
   "source": [
    "The cell below contains a subset of the most important predictors found using the RandomForest. Using these predictors results in **two** clusters that are highly concentrated in ECD readings, each having a  distinguishable shape. They seem to be more 'pure' then the 'ecd' cluster obtained when we used the predictors in the cell above. In fact here is their distribution:\n",
    "* 177 ECDs & 280 unsuccessful\n",
    "* 160 ECDs & 22 unsuccessful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21bbea-4f8c-4443-996d-94855d3e7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_subset = pred[['list of 12 predictors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd05ec9-ed64-460a-86bc-100cd6b512ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_pred = ts.merge(pred_subset, on = 'TestId', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e850b2c-4c3c-4e38-a59c-7c94790ad98e",
   "metadata": {},
   "source": [
    "## **Run KShape Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfec9ad-6ab5-4668-923e-899786020d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this method to operate properly, prior scaling is required (mean = 0, standard deviation = 1)\n",
    "X = TimeSeriesScalerMeanVariance().fit_transform(ts_pred.drop(columns = ['TestId', 'Label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fd90c0-bdbc-4edd-9c0a-ce0d05661e3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "`init = 'random'` : [\"Choose k observations (rows) at random from data for the initial centroids\"](https://tslearn.readthedocs.io/en/stable/gen_modules/clustering/tslearn.clustering.KShape.html)\n",
    "\n",
    "`n_init = 10`: [\"Number of time the k-Shape algorithm will be run with different centroid seeds\"](https://tslearn.readthedocs.io/en/stable/gen_modules/clustering/tslearn.clustering.KShape.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3aa29-98bf-4952-a8f0-96d843d8bb0b",
   "metadata": {},
   "source": [
    "Having an `n_init = 10` results in the algorithm running for a lot more time than if `n_init = 1`, but hopefully yields better results because the output will correspond to the iteration that led to the smallest inertia. Inertia is the sum of squared distances between sequences in a specific cluster and their corresponding centroid [(see reference here)](https://www.codecademy.com/learn/machine-learning/modules/dspath-clustering/cheatsheet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152d3e4-be28-410c-80e1-b5356920f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the algorithm and getting the corresponding cluster number for each reading\n",
    "# For the sake of this example, set n_init = 1\n",
    "clusters = KShape(n_clusters = 30, n_init = 1, init='random').fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6544e-4569-4643-94d3-e922300db9a7",
   "metadata": {},
   "source": [
    "## **Diagnostics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b831e4d-3ba2-489c-ba50-a27de258e507",
   "metadata": {},
   "source": [
    "First, we have to get the data into the proper format so that it can be fed to the other diagnostic functions that are in the `diagnostic.py` file. The `prepare_data` function returns a dataframe that contains all the timeseries and predictors data as well as the corresponding cluster for each testid. Also, since we had previously appended a column containing the labels ('un', 'wild', 'syn', 'con') to the predictor file (`pred`), this column will also be in `diagnostic_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2445eea-eb36-4dbc-83d7-6b12b522637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_df = prepare_data(ts_df = ts, pred_df = pred, clusters = clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b047e-a210-4a9c-b487-1bee942dff56",
   "metadata": {},
   "source": [
    "### **Get label counts for each cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d95f4-7b8b-456b-b07d-f4fff8639e84",
   "metadata": {},
   "source": [
    "The function `get_label_counts` retrieves the number of readings from different categories (contained in the `label_col`) in each cluster (eg. number of unsuccessful, number of wild ECD errors, etc) and store them in a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419fafda-ad92-4e8a-8764-03e66e4f2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_label_counts(ts_pred = diagnostic_df, clust_col = 'Cluster', label_col = 'Label').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433dddc-ea24-458e-8a98-ce38e64e2399",
   "metadata": {},
   "source": [
    "### **Describe clusters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd1f47-5ca2-48e5-9a60-36131e025435",
   "metadata": {},
   "source": [
    "The function `describe_clusters` prints the number of readings from different categories in each cluster (eg. number of unsuccessful, number of wild ECD errors, etc). It also displays histograms for the desired aggregate predictors in each cluster. If `show_traces = True`, it plots the waveforms in each cluster, with one plot for unsuccessful, and one for the various kinds of ECD errors. The clusters that contain the highest number of ECD errors are displayed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da8da0-bbea-47a8-98ff-4a5c78b0accf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "describe_clusters(ts_pred = diagnostic_df, feature_list = ['Label', 'ReturnCode'], show_traces = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af629fe-ba6f-4ffc-acb3-bdeeafb1985c",
   "metadata": {},
   "source": [
    "If we are interested in comparing the distributions of certain aggregate predictors we can also do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0ec3d-1352-477f-ad25-f9bfa3c4a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_cluster_densities(ts_pred = diagnostic_df, clust1 = 6, clust2 = 17, \n",
    "                          feature_list = ['CMean', 'PMean', 'SMean'], clust_col = 'Cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d287e7c-d8a2-4d1b-aaec-565379b648fc",
   "metadata": {},
   "source": [
    "# **Centroids**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce042fa-e2b5-4188-9b9e-fb8764519567",
   "metadata": {},
   "source": [
    "Since the dataframe used in the KShape algorithm includes both timeseries data and the most important predictors from the predictor file, the centroids that we obtain can't be expressed as a function of time (on the x-axis). Also, the centroid of each cluster is the [\"eigenvector that corresponds to the largest eigenvalue\"](http://www1.cs.columbia.edu/~jopa/Papers/PaparrizosSIGMOD2015.pdf) of a matrix found by maximizing the [\"sum of squared similarity [between the centroid] and all the other timeseries sequences\"](http://www1.cs.columbia.edu/~jopa/Papers/PaparrizosSIGMOD2015.pdf). This means that it is not clear what the centroid represents when combining aggregate predictors and timeseries data.\n",
    "\n",
    "This being said, had we only used timeseries data for the clustering, the centroid would have effectively represented the \"summary\" shape found in the corresponding cluster. \n",
    "\n",
    "The code below can be used in the case where clustering is performed only with timeseries data and allows the user to create:\n",
    "* A dataframe containing the centroid for each cluster using `format_centroids`\n",
    "* Diagnostic plots comparing the waveforms in a cluster and its corresponding centroid shape using `centroid_diagnostic`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3015ef05-2139-4c49-8828-57510448f293",
   "metadata": {},
   "source": [
    "#### **Performing KShape only with timeseries data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4b256-f163-46eb-99ad-0b05d12e9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of centroids had we used only timeseries data to cluster\n",
    "\n",
    "# Dataframe containing only the timeseries data (columns with numeric names as columns) and the Label column\n",
    "ts_only = ts_pred.loc[:,ts_pred.columns[~ts_pred.columns.str.isalpha()]]\n",
    "ts_only = pd.concat([ts_pred['Label'], ts_only], axis = 1)\n",
    "\n",
    "# For this method to operate properly, prior scaling is required (mean = 0, standard deviation = 1)\n",
    "X = TimeSeriesScalerMeanVariance().fit_transform(ts_only.drop(columns = ['Label']))\n",
    "\n",
    "# Here we are running `fit` and `predict` seperately instead of the most efficient `fit_predict` because we need the information contained in \n",
    "# `.cluster_centers_` to plot the centroids.\n",
    "\n",
    "# Running the algorithm and getting the corresponding cluster number for each reading\n",
    "# n_init = 1 just for the sake of the example (shorter to run), but suggest increasing in case of actual use\n",
    "k_shape = KShape(n_clusters = 30, n_init=1, init='random').fit(X)\n",
    "clusters_centroid = k_shape.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd79e8-f5cd-401a-9aca-d218fcce51b8",
   "metadata": {},
   "source": [
    "#### **Obtain the centroids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bee4ff-dbc4-4784-aee4-74afda4e8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Outputs a dataframe containing the centroid for each cluster. Each column represents a centroid for a specific cluster. Each row is a timestamp.\n",
    "    \n",
    "    Args:\n",
    "        data: The dataframe that was initially passed when fitting the KShape algorithm (as the fit_transform argument). \n",
    "        fit: The object containing the output from the .fit() method. Contains the attribute `cluster_centers_`\\\n",
    "        clusters: Numpy array containing the labels for the clusters formed from the algorithm. The output from the `.predict()` statement\n",
    "        \n",
    "    Returns:\n",
    "        Pandas dataframe containing the centroids. The dimensions should be [number of clusters, number of timestamps].\n",
    "\"\"\"\n",
    "def format_centroids(data, fit, clusters):\n",
    "    # Going from a 3D numpy array (where 3rd dimension is 1), to a 2D array\n",
    "    centroids = pd.DataFrame(fit.cluster_centers_[:,:,-1])\n",
    "    name_columns = data.columns\n",
    "    \n",
    "    # Renaming the columns\n",
    "    centroids.columns = name_columns\n",
    "    \n",
    "    # Keeping only the columns that correspond to a timestamp (excludes the predictors)\n",
    "    name_columns_timestamp = name_columns[~name_columns.str.isalpha()]\n",
    "    \n",
    "    centroids_df = centroids.loc[:,name_columns.isin(name_columns_timestamp)] \n",
    "    centroids_df['Cluster'] = np.sort(np.unique(clusters))\n",
    "    \n",
    "    return centroids_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626158a7-0abb-46ea-9299-6234c4355aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = format_centroids(data = ts_only.drop(columns = ['Label']), fit = k_shape, clusters = clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0baa7-c217-462a-a629-6b685293d039",
   "metadata": {},
   "source": [
    "#### **Diagnostics for the centroid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc48ec4-24d5-4e9b-a16b-93fedaec2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_df_centroid = prepare_data(ts_df = ts, pred_df = pred, clusters = clusters_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd37ca4-ccf3-4342-9c30-039831463e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prints the total number of ECD and unsuccessful readings in the cluster as well as to plot.\n",
    "    The first one shows the traces of each reading in that cluster and the second one shows its centroid.\n",
    "    Args:\n",
    "        diagnostic_df: A data frame containing the time series data and predictors as well as a column corresponding to the clusters (use the df output by `prepare_data` function).\n",
    "        centroid_df: Pandas dataframe containing the centroids (output by the `format_centroids` function). \n",
    "        clust_col: The name of the column containing cluster labels. \n",
    "        label_col: The name of the column cotaining the data labels ('un'/'syn'/'cont'/etc) \n",
    "        \n",
    "    Returns:\n",
    "        Diagnostic information relating to the centroids.\n",
    "\"\"\"\n",
    "def centroid_diagnostic(diagnostic_df, centroid_df, clust_col = 'Cluster', label_col = 'Label'):\n",
    "    \n",
    "    # Getting the names of the columns that are numeric/correspond to timeseries \n",
    "    name_columns = diagnostic_df.columns[~diagnostic_df.columns.str.isalpha()]\n",
    "    \n",
    "    # Here we remove the last 4 seconds because there seems to be some distortion\n",
    "    # at the extremities. More research needs to be done to understand the root cause  of this distortion\n",
    "    name_columns = name_columns[0:int(len(name_columns)-(4/0.2))] \n",
    "    \n",
    "    # Dataframe containing only the timeseries data and a column with corresponding to the assigned cluster\n",
    "    timeseries = diagnostic_df.loc[:,name_columns]\n",
    "    timeseries = pd.concat([timeseries, diagnostic_df[clust_col]], axis = 1)\n",
    "    \n",
    "    # Removing the last 4 seconds because of the distortions \n",
    "    clusters = centroid_df[clust_col] \n",
    "    centroid_df = centroid_df.loc[:, name_columns]\n",
    "    centroid_df = pd.concat([centroid_df, clusters], axis = 1)\n",
    "    \n",
    "    start = timeseries.columns[0]\n",
    "    end = timeseries.drop(columns = clust_col).columns[-1]\n",
    "    \n",
    "    counts = get_label_counts(diagnostic_df, clust_col = clust_col, label_col = label_col)\n",
    "    \n",
    "    # Printing the number of readings for each label\n",
    "    for cluster in counts.index:\n",
    "        # Print the cluster number: \n",
    "        print('\\nCluster', cluster, '\\n------------------------------------')\n",
    "        # Subset the data frame with just the cluster.\n",
    "        clust_subset = diagnostic_df[diagnostic_df[clust_col] == cluster]\n",
    "        # Print some summary information\n",
    "        for label in ['tot_ecds', 'un']:\n",
    "            print(\"Number of\", label, \":\", counts[label][cluster])\n",
    "    \n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize = (75, 15))\n",
    "        axes[0].plot(timeseries[timeseries[clust_col] == cluster].drop(columns = clust_col).transpose())\n",
    "        axes[1].plot(centroid_df.drop(columns = clust_col).columns, centroid_df.drop(columns = clust_col).iloc[cluster])\n",
    "\n",
    "        # Formatting the plots\n",
    "        for i in range(2):\n",
    "            axes[i].set_xlabel('time (secs) w.r.t sample detect', fontsize = 30)\n",
    "            axes[i].xaxis.set_ticks([start,'-0.0', end])\n",
    "            axes[i].tick_params(axis='x', labelsize=25)\n",
    "            axes[i].tick_params(axis='y', labelsize=25)\n",
    "\n",
    "        axes[0].set_ylabel('signal', fontsize = 30)\n",
    "        axes[1].set_ylabel('centroid', fontsize = 30)\n",
    "\n",
    "        axes[0].set_title(f'Readings in cluster {cluster}', fontsize = 35)\n",
    "        axes[1].set_title(f'Centroid for cluster {cluster}', fontsize = 35)\n",
    "\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c420e43-20fd-46a8-a821-21e18025743e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "centroid_diagnostic(diagnostic_df = diagnostic_df_centroid, centroid_df = centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c4037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
