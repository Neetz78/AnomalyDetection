{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cfbe60-f61f-4fd5-b4f1-82648d20da85",
   "metadata": {},
   "source": [
    "# **PCA Notebook Description**\n",
    "\n",
    "Description: This notebook gives an example of running a principle component analysis on the smoothed and normalized data to get visualizations in a lower dimension space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b91a2b-4317-4686-9dcb-fc5b5d87254f",
   "metadata": {},
   "source": [
    "## **Package Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f12274-496c-494a-b4be-ffd57459702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead518d6-a524-4e49-95ec-e590e2c21eb6",
   "metadata": {},
   "source": [
    "## **Read in the data**\n",
    "\n",
    "**This is the data that was subset to -30 to 40 seconds w.r.t sample detect time, normalized (scaled between 0 and 1), and smoothed with a convolution (bartlett window of length 50)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18286b39-1a47-4df1-ba5d-670f97087919",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecd_ts = pd.read_csv('../Data/PreprocessedData/TimeSeries/ecd_smooth.csv')\n",
    "syn_ts = pd.read_csv('../Data/PreprocessedData/TimeSeries/syn_smooth.csv')\n",
    "cont_ts = pd.read_csv('../Data/PreprocessedData/TimeSeries/cont_smooth.csv')\n",
    "un_ts = pd.read_csv('../Data/PreprocessedData/TimeSeries/un_smooth.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cfe4a-87cc-4d76-adc5-71342a02dc3a",
   "metadata": {},
   "source": [
    "Save the ids for each category in case we need them later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e43e4-ba9d-4a88-918d-704a2dc31223",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecd_ids = ecd_ts['TestId']\n",
    "un_ids = un_ts['TestId']\n",
    "cont_ids = cont_ts['TestId']\n",
    "syn_ids = syn_ts['TestId']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4a72b-fc62-4d6a-8774-9a44406c4da8",
   "metadata": {},
   "source": [
    "Make a new data frame with all of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d451f9ec-c20b-4239-a328-b3b54c0934c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ts = pd.concat([un_ts, ecd_ts, cont_ts, syn_ts]).drop('TestId', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e48a871-1939-4f58-80da-5633546e4215",
   "metadata": {},
   "source": [
    "Create labels for the ECD errors vs. unssuccessful. This will allow colour coding in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e243047-1d16-4859-bec2-bd867b4a50be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = {\"pc\":\"red\", \"un\":\"blue\"}\n",
    "pc_lab = pd.Series(['pc'])\n",
    "un_lab = pd.Series(['un'])\n",
    "x = un_lab.repeat(len(un_ts))\n",
    "y = pc_lab.repeat(len(ecd_ts) + len(cont_ts) + len(syn_ts))\n",
    "labs = pd.concat([x, y])\n",
    "all_ts.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c37fc-6285-45d1-ba6c-357179559c50",
   "metadata": {},
   "source": [
    "## **Run the PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab6299-375f-49d9-9fb2-f2e424d4e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep enough components to account for 95% of the variation in the data. \n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Run the PCA on standardized data to avoid any particualy time point with a lot of variation having too much sway.\n",
    "principalComponents = pca.fit_transform(StandardScaler().fit_transform(all_ts))\n",
    "\n",
    "# Make a data frame with the resulting components. \n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['Component '+ str(i+1) for i in range(pca.n_components_)])\n",
    "\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c8d40-deb2-4cbd-b954-9897d9c59af5",
   "metadata": {},
   "source": [
    "Here we can see that two components are sufficient to explain 95% of the variance in our time series. This is convenient as we can easily visualize things in a two-dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3201725-4646-41f1-9832-56baa146d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the predefined data labels into the PCA data frame. \n",
    "principalDf['label'] = labs.reset_index(drop=True)\n",
    "principalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d9bb2-7307-4186-b16d-8eb6204c685e",
   "metadata": {},
   "source": [
    "## **Visualize the PCA**|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5d892-fc53-413a-9baf-a678f335128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = principalDf\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['un', 'pc']\n",
    "colors = ['b', 'r']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['label'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'Component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'Component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baaf9d8-99ca-4d5c-9658-228a52a4f141",
   "metadata": {},
   "source": [
    "Here we can see that most of the ECDs load around 0 for the second principle components.\n",
    "This is interesting as it provides some indication that there might be a way to pull them out. Next, lets look at the eigenvectors to see the coeffecients for different time points in the different components. This will help us see which time points explain most of the variation in each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f56c7d-552a-4690-81e4-5bfb8112d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The matrix of variable loadings (i.e., matrix whose columns contain the eigenvectors)\n",
    "# The eigenvectors provide the coefficients for the linear combination. This will tell use which time points are \n",
    "# most influential. \n",
    "rotation = pd.DataFrame(pca.components_, columns = all_ts.columns).T\n",
    "rotation.columns = [f\"PC_{i}\" for i in range(1, len(pca.components_) + 1)]\n",
    "rotation\n",
    "\n",
    "# Take a look at the first eigenvector (coefficients for the first principle component). \n",
    "rotation.sort_values(by=['PC_1'], key = abs, ascending = False).head(30)[['PC_1']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992f8c4-8f85-4991-b882-ad0391c9bb93",
   "metadata": {},
   "source": [
    "Interestingly, the predictors with the largest coefficients for principal component 1 are all between 11-15 seconds, around what is considered to be the post window. Now let's take a look at the second eigenvector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e108d7-d930-44dc-9652-de6ce869acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation.sort_values(by=['PC_2'], key = abs, ascending = False).head(30)[['PC_2']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c9233-102f-4664-a7c5-5b848e460fdf",
   "metadata": {},
   "source": [
    "The predictors with the largest coefficients for principal component 2 are all between 35-40 seconds, around what is considered to be the sample window. Now let's visualize these two eigenvectors to get a clearer idea of what's going on and what these eigenvectors might correspond to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a5f7c-6b88-44b5-9179-958a3797b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(-30,40,0.2),rotation['PC_1'], label = 'eigenvector 1')\n",
    "plt.plot(np.arange(-30,40,0.2),rotation['PC_2'], label = 'eigenvector 2')\n",
    "plt.xlabel('time w.r.t sample detect time (secs)')\n",
    "plt.ylabel('coefficient')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7e765-39ef-4f68-97a5-37690aa97e70",
   "metadata": {},
   "source": [
    "We can see that coefficients increase after sample detect for principle component 2 but not principle component 1. This means that readings that score low on principle component 2 (ECDs) are less explained by what happens after sample detect time. This is very interesting given that we expect ECD errors to remain around 0 (unchanged relative to calibration) after sample detection. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
