{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bf8174",
   "metadata": {},
   "source": [
    "# **Autoencoder and Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739757c4",
   "metadata": {},
   "source": [
    "[\"Autoencoders are a type of artificial neural network used to learn efficient data patterns in an unsupervised manner\"](https://www.sciencedirect.com/topics/engineering/autoencoder#:~:text=An%20autoencoder%20is%20a%20type,by%20a%20hidden%20layer%20h.). An autoencoder ideally consists of an encoder and decoder.\n",
    "- Encoder: Used to encode the raw waveform and reduce it to a smaller dimension.\n",
    "- Decoder: Used to reconstruct the original waveform with the encoded data.\n",
    "\n",
    "Important features of an autoencoder:\n",
    "- Noise reduction\n",
    "- Dimensionality Reduction\n",
    "- Feature Extraction\n",
    "\n",
    "To obtain better clustering results, we will use both the timeseries data (waveforms) and the aggregate predictors file. Here is the pipeline used in this notebook:\n",
    "\n",
    "1. **Use an encoder on the timeseries data (from -30 to 40 w.r.t sample detect, normalized and smoothed) for feature extraction**\n",
    "2. **Perform dimension reduction on these features using PCA to explain 95% of the cumulative variance**\n",
    "3. **Concatenante the most important predictors (found using the RandomForest Classifier variable importance) to the principal components obtained in step 2**\n",
    "4. **Perform clustering using Gaussian Mixture Modelling**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c8b5a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bdb7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd                                  \n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import random as python_random\n",
    "\n",
    "from numpy.random import seed                       \n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.layers import Input, Dense                # keras function for defining the embedded layers within the autoencoder\n",
    "from keras.models import Model                       # to model the autoencoder      \n",
    "from sklearn.mixture import GaussianMixture          \n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "# Our script to plot various diagnostic plots\n",
    "from diagnostics import *\n",
    "\n",
    "np.random.seed(123)                                 \n",
    "python_random.seed(123)                             \n",
    "tf.random.set_seed(1234)  \n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "# Removes the warning when adding a column to df\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d1849b",
   "metadata": {},
   "source": [
    "## Read in the preprocessed data to be clustered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ba55d1",
   "metadata": {},
   "source": [
    "#### Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f52115",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecd_ts = pd.read_csv(\"../Data/PreprocessedData/TimeSeries/ecd_smooth.csv\")\n",
    "syn_ts = pd.read_csv(\"../Data/PreprocessedData/TimeSeries/syn_smooth.csv\")\n",
    "con_ts = pd.read_csv(\"../Data/PreprocessedData/TimeSeries/cont_smooth.csv\")\n",
    "un_ts = pd.read_csv(\"../Data/PreprocessedData/TimeSeries/un_smooth.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae1d8d",
   "metadata": {},
   "source": [
    "#### Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7620f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_pred = pd.read_csv('../Data/PreprocessedData/Predictors/Unsuccessful.csv')\n",
    "ecd_pred = pd.read_csv('../Data/PreprocessedData/Predictors/ecdContact.csv')\n",
    "syn_pred = pd.read_csv('../Data/RawData/Predictors/SyntheticECD.csv')\n",
    "con_pred =  pd.read_csv('../Data/RawData/Predictors/ECDAggContaminated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ca0b78",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79408e0",
   "metadata": {},
   "source": [
    "Here we are adding labels to the timeseries and predictor files (i.e, 'ecd', 'syn'). Then we are merging them together to obtain one dataframe with all the timeseries data and another with all the predictor data. Finally, we are creating a dataframe containing both our timeseries and predictor data for each TestId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding labels for predictor data\n",
    "un_pred['Label'] = \"un\"\n",
    "ecd_pred['Label'] = \"ecd\"\n",
    "syn_pred['Label'] = \"syn\"\n",
    "con_pred['Label'] = \"con\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding lables for timeseries data\n",
    "un_ts['Label'] = \"un\"\n",
    "ecd_ts['Label'] = \"ecd\"\n",
    "syn_ts['Label'] = \"syn\"\n",
    "con_ts['Label'] = \"con\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging the timeseries data into a single dataframe.\n",
    "ts = pd.concat([ecd_ts, syn_ts, con_ts, un_ts])            \n",
    "ts = ts.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53414669",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging the predictor files and renaming the TestID column to match with timeseries file.\n",
    "preds = pd.concat([un_pred, ecd_pred, syn_pred, con_pred])\n",
    "preds = preds.rename({'TestID':'TestId'}, axis = 1)\n",
    "\n",
    "# Because we preprocessed the timeseries data, some of the ids are in the predictor file but no longer in the timeseries data\n",
    "# Thus, we just want to keep the testid that are common to both\n",
    "preds = pd.DataFrame(ts['TestId']).merge(preds, on = 'TestId', how = 'left')\n",
    "preds = preds.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1553ac-f044-4812-a56d-b9956cec5345",
   "metadata": {},
   "source": [
    "After obtaining a promising cluster (using KMeans) that contained lots of ecd contacts, we asked Olivia to verify if the unsuccessful readings that were in that same cluster were ecd contacts. It turned out that 35 out of the 50 actually were. The other were linked to errors for another analyte (sample bubble or ecd contact). Thus, we changed the label of these unsuccessful readings to `modified_ecds` instead of `un`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f2b30-40c7-487c-a835-c385d866f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the label from unsuccessful to 'modified_ecd' for the TestIds we were confirmed by Olivia were actually ecd contact errors\n",
    "un_ids_checked = pd.read_csv('../Data/RawData/un_ids_checked.csv')\n",
    "\n",
    "preds = preds.merge(un_ids_checked, on = 'TestId', how = 'left')\n",
    "ts = ts.merge(un_ids_checked, on = 'TestId', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9868b-66ee-4786-9d4c-2f64171492f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(len(preds)):\n",
    "    if (preds['Label'][row] == 'un') & (preds['ecd'][row] == 'Yes'):\n",
    "        preds['Label'][row] = 'modified_ecd'\n",
    "    \n",
    "for row in range(len(ts)):\n",
    "    if (ts['Label'][row] == 'un') & (ts['ecd'][row] == 'Yes'):\n",
    "        ts['Label'][row] = 'modified_ecd'\n",
    "\n",
    "preds = preds.drop(columns = 'ecd')\n",
    "ts = ts.drop(columns = 'ecd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbf5c4",
   "metadata": {},
   "source": [
    "After wrangling, we have three dataframes:\n",
    "* ts : Dataframe which contains only the smoothed (convolution filter) and normalized (btw 0 and 1) waveforms from -30 to 40 (w.r.t sample detect time) \n",
    "\n",
    "* preds : Dataframe which contains all the aggregate predictors (only for the TestIds that are also in ts).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff3766",
   "metadata": {},
   "source": [
    "## **1. Extracting features using an autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4389acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the labels from the data to feed in to the autoencoder.\n",
    "X = ts.drop(columns = ['TestId', 'Label']) \n",
    "Y = ts['Label']   \n",
    "\n",
    "# Storing the TestIds in a separate variable\n",
    "ids = ts['TestId'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce658485-e9c5-4954-9b67-52cb9d3aa5ea",
   "metadata": {},
   "source": [
    "We need to split the data into a training and testing dataset. This is necessary as we need to train the autoencoder on the training set to learn to extract features from the raw data, and then validate the results using the testing set. The autoencoder has to be trained using both the encoder and the decoder, but then we can only use the encoder part to extract the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d773a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle = (True) to shuffle the data, so a diverse set is chosen instead of the same labeled ones.\n",
    "    \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668721f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of features\n",
    "n_col = X.shape[1]                                       \n",
    "\n",
    "# Defining encoding dimensions\n",
    "# Since we are only extracting features we have chosen 349, as we do not want to reduce the dimensionality of the data.\n",
    "encoding_dim = 349                                        \n",
    "\n",
    "# Defining the input_dim which needs to be equal to output_dim\n",
    "# Important: The input and output dimensions should always be the same \n",
    "input_dim = Input(shape = (n_col, ))                      \n",
    "\n",
    "# Defining the encoding layers : creating many layers improves the model's ability to extract better features.\n",
    "# relu function : a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.\n",
    "# This is the encoder because the depth of the layers are decreasing (for dimension reduction)\n",
    "en1 = Dense(3000, activation = 'relu')(input_dim)\n",
    "en2 = Dense(2750, activation = 'relu')(en1)\n",
    "en3 = Dense(2500, activation = 'relu')(en2)\n",
    "en4 = Dense(2250, activation = 'relu')(en3)\n",
    "en5 = Dense(2000, activation = 'relu')(en4)               \n",
    "en6 = Dense(1750, activation = 'relu')(en5)               \n",
    "en7 = Dense(1500, activation = 'relu')(en6) \n",
    "en8 = Dense(1250, activation = 'relu')(en7)\n",
    "en9 = Dense(1000, activation = 'relu')(en8)\n",
    "en10 = Dense(750, activation = 'relu')(en9)\n",
    "en11 = Dense(500, activation = 'relu')(en10)\n",
    "en12 = Dense(250, activation = 'relu')(en11)\n",
    "en13 = Dense(encoding_dim, activation = 'relu')(en12)\n",
    "\n",
    "# Defining the Decoder Layers (same number of layers as the encoder)\n",
    "# Parameters are chosen after mulitple trail and error runs\n",
    "# This is the decoder because the depth of the layers are increasing (for reconstruction)\n",
    "de1 = Dense(250, activation = 'relu')(en13)               \n",
    "de2 = Dense(500, activation = 'relu')(de1)                 \n",
    "de3 = Dense(750, activation = 'relu')(de2)                \n",
    "de4 = Dense(1000, activation = 'relu')(de3)\n",
    "de5 = Dense(1250, activation = 'relu')(de4)\n",
    "de6 = Dense(1500, activation = 'relu')(de5)\n",
    "de7 = Dense(1750, activation = 'relu')(de6)\n",
    "de8 = Dense(2000, activation = 'relu')(de7)\n",
    "de9 = Dense(2250, activation = 'relu')(de8)\n",
    "de10 = Dense(2500, activation = 'relu')(de9)\n",
    "de11 = Dense(2750, activation = 'relu')(de10)\n",
    "de12 = Dense(3000, activation = 'relu')(de11)\n",
    "de13 = Dense(n_col, activation = 'sigmoid')(de12)\n",
    "\n",
    "# Combining both encoding and decoding layers\n",
    "autoencoder = Model(inputs = input_dim, outputs = de13) \n",
    "\n",
    "# Compiling the Model\n",
    "autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef1ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary provides us with the potential model and the number of possible parameters from the raw time series\n",
    "# autoencoder.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0918f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the autoencoder on X_train and validating on the X_test dataset,\n",
    "# parameters:\n",
    "    # epochs = 10 (can be increased to futher reduce the loss from the model)\n",
    "    # batch_size = 32  (for every iteration the data chosen is of the size 12, can be further tuned)\n",
    "\n",
    "autoencoder.fit(X_train, X_train, epochs = 10, batch_size = 32, shuffle = False, validation_data = (X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8faa2c1",
   "metadata": {},
   "source": [
    "Now that the autoencoder is fully trained, we can use only the encoder part to extract features from the timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b9765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output is the last layer of the encoding part (here en13)\n",
    "encoder = Model(inputs = input_dim, outputs = en13)\n",
    "\n",
    "# Inputting the encoding_dim which has the number of dimensions required\n",
    "encoded_input = Input(shape = (encoding_dim, )) \n",
    "\n",
    "# Now we pass in the entire dataset X with the predict function on the model which is trained by the autoencoder to extract features.\n",
    "features = pd.DataFrame(encoder.predict(X)) \n",
    "\n",
    "# Adding the column names for the extracted features as feature_*\n",
    "features = features.add_prefix('feature_') \n",
    "\n",
    "# Adding the labels and TestIds back to the newly created dataframe features\n",
    "features['Label'] = Y                                       \n",
    "features['TestId'] = ids                                     \n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed29cda",
   "metadata": {},
   "source": [
    "We can see that a few of the columns contain only zeros. Hence, they don't add any value and will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb55e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing those features from the dataframe that have all the values set to zero has not much insight is gathered from this set.\n",
    "data_without_zero_features = features.loc[:, (features != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17c4dd",
   "metadata": {},
   "source": [
    "## **2. Performing Principal Component Analysis(PCA) for dimension reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2ac69",
   "metadata": {},
   "source": [
    "We perform PCA to reduce the number of features (~350 here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1ad44-32d2-4dd8-8f22-0415ff25b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_zero_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72873e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the \"predictors\" (X) and the \"response\" variable (Y)\n",
    "X_pca = data_without_zero_features.drop(columns = ['Label', 'TestId'])          \n",
    "Y_pca = data_without_zero_features[[\"TestId\",'Label']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iteratively running PCA to see the cumulative variance , which helps to decide the number of components to choose for further steps.\n",
    "\n",
    "def pca_plot(data):\n",
    "    '''\n",
    "    Function to determine how many components to choose based on the cumulative variance explained.\n",
    "    Args:\n",
    "        data : The dataframe on which PCA needs to be performed.\n",
    "    '''\n",
    "    pca = PCA().fit(data) \n",
    "    # Plotting the cumulative variance explained with number of components as x axis and variation explained on y axis.\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))  \n",
    "    plt.xlim(0,20,1)                                  \n",
    "    plt.xlabel('Number of components')                  \n",
    "    plt.ylabel('Cumulative variance explained')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed15d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_plot(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89071f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the threshold to 0.95, i.e choose only those components that account for a variation of 95% \n",
    "\n",
    "def pca_components(data, percent = 0.95):\n",
    "    '''Function to get Principal Components Analysis based on the number of components or the threshold passed.\n",
    "    Args :\n",
    "        data : Predictors on which the PCA needs to be applied on.\n",
    "        percent : The percent of variation we want to account for with the PCA or the number of components we wish to obtain.\n",
    "    Returns:\n",
    "        Dataframe with the PCA.\n",
    "    '''\n",
    "    pca = PCA(n_components = percent)                             \n",
    "    pc = pca.fit_transform(data)                                \n",
    "    component_names = [f\"PC{i+1}\" for i in range(pc.shape[1])]  \n",
    "    newdata = pd.DataFrame(pc, columns=component_names)         \n",
    "    return newdata                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pca_components(X_pca,0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda06ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the TestId and Label back to the new dataframe pca_df created\n",
    "pca_df = pd.concat([pca_df, Y_pca], axis = 1)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcead390",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the first two PC components to visualize the points\n",
    "alt.Chart(pca_df).mark_circle(size=60).encode(\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='Label'\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ea4c8",
   "metadata": {},
   "source": [
    "## **3. Concatenate the most import predictors to the principal components obtained above**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a6946-7380-400e-a5d8-f90965453f86",
   "metadata": {},
   "source": [
    "Here, we use the predictors (from the aggregate predictors file) that scored the highest in term of variable importance when running a RandomForest Classifier.\n",
    "\n",
    "These were the predictors that were kept:\n",
    "\n",
    "- 12 aggregate predictors (deleted here for confidentiality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd35a6-7272-4ec9-907f-13959e41770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_subset = preds[['list of 12 predictors']]\n",
    "pred_subset = pred_subset.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f901eb",
   "metadata": {},
   "source": [
    "Now that we have two dataframes ready one with the principal components (pca_df) and one with aggregate predictors (pred_subset),we combine them together to form our final predictors for the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5edb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropecdg the `Label` column because it is already in the pca_df\n",
    "pca_pred = pca_df.merge(pred_subset.drop(columns = 'Label'), on = 'TestId', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f9e4d-ba54-4903-8cd3-09cf68add5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the TestId and Label in a separate dataframe.\n",
    "Y_pca_pred = pca_pred[['Label','TestId']]                         \n",
    "\n",
    "# Dropecdg them from the final predictors\n",
    "pca_pred = pca_pred.drop(columns = ['Label','TestId']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b10fcf",
   "metadata": {},
   "source": [
    "## **4. Clustering using Gaussian Mixture Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10083da",
   "metadata": {},
   "source": [
    "Approach which assumes every data point belongs to a different cluster with a certain probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac3b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot the BIC scores for different numbers of clusters. This helps us choose the number of clusters based on the lowest BIC score. \n",
    "\n",
    "def cluster_plot(data,n):\n",
    "    '''\n",
    "    Function to plot the number of components with BIC scores to choose the ideal number of clusters.\n",
    "    Args :\n",
    "        data : Dataframe on which clustering needs to be performed\n",
    "        n : The number of potential clusters \n",
    "    '''\n",
    "    ## to select an inital number of components\n",
    "    n_components = np.arange(1, n)\n",
    "    \n",
    "    ## applying gaussian mixture modeling \n",
    "    models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(data) for n in n_components]\n",
    "    plt.plot(n_components, [m.bic(data) for m in models], label='BIC')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Number of clusters');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e637e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To select the number of potential cluster, running an iterative setup to see the number of cluster with minimum BIC and AIC value i.e BIC measures the maximum likehood among the points.\n",
    "cluster_plot(pca_pred, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf49d0",
   "metadata": {},
   "source": [
    "The smallest value of BIC occurs when the number of clusters is approximately 20. There does seem to be an elbow at approximately 5, but after trying this amount of clusters, we noticed that a lot of different shapes were being clustered together, hence we decided to increase the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fitting the Gaussian mixture model\n",
    "\n",
    "def gaussian_fit(data, n):\n",
    "    '''\n",
    "    Function to fit a gaussian mixture model \n",
    "    Args :\n",
    "        data : Dataframe on which clustering needs to be performed\n",
    "        n : The number of potential clusters \n",
    "    \n",
    "    Returns : The input dataframe (data) with an appended column for the corresponding cluster labels obtained\n",
    "    \n",
    "    '''\n",
    "    gmm = GaussianMixture(n_components=n)                              \n",
    "    gmm.fit(data)                                                      \n",
    "    labels = gmm.predict(data) \n",
    "    # Assigning the Cluster to a column in the dataframe\n",
    "    data['Cluster'] = labels                                          \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_df = gaussian_fit(pca_pred, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging back the TestId and Label\n",
    "gaussian_df = pd.concat([gaussian_df, Y_pca_pred], axis = 1)                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a014a4b-d12e-4145-aacb-1465d3734443",
   "metadata": {},
   "source": [
    "## **Diagnostics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b967b-970d-4f6c-baed-15bcbb98bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including the cluster label to the timeseries data\n",
    "ts_df = ts.merge(gaussian_df[['TestId', 'Cluster']], on = 'TestId', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f62cb-28ce-498c-9a51-a61f318b1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropecdg Label colu,m from ts_df because it is already in pred_df\n",
    "diagnostic_df = prepare_data(ts_df = ts_df.drop(columns = 'Label'), pred_df = preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f1bd4-78e2-4f5b-96df-8349c7701701",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_label_counts(ts_pred = diagnostic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd816e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_clusters(ts_pred = diagnostic_df, feature_list = ['AggPred1', 'AggPred2'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
