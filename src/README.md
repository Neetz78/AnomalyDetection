# **Glossary**

Here is the glossary for the pipelines we produced in our second iteration. Of these pipelines, the autoencoder and KShape algorithms yielded the best results in terms of isolatig ECDs, and are probably what should be used in the future. 

## **Preprocessing**

Preprocessing steps taken in the first iteration. The notebooks are listed in the order that they were used. 


|Notebook/Script| Description |
|--------|--------------------------------------|
| [Preprocessing](code/preprocessing.ipynb) | <ul><li> Executing all of the cells in this notebook will generate the preprocessed files needed to proceed with clustering. </li><li>This notebook contains functions that window, normalize, and smooth the data:<ul><li>Windowing Functions</li><ul><li> **window_and_zero** - sets sample to detect time to 0 and allows the user to subset the waveform relative to sample detect time (eg. 30 seconds before to 40 seconds after)</li><li>**window_after_zeroed** - once the waveforms have already been zeroed at sample detect, this function can be used to window them with respect to sample detect time. Useful for getting the calibration/post/sample windows</li></ul></ul><ul><li>Scaling Functions</li><ul><li>**normalize** - scales the time series in a dataframe so that their values are between 0 and 1.</li><li>**standardize** - standardizes all the time series in a dataframe (transform each time series to have mean 0 and standard deviation 1).</li></ul></ul><ul><li>Smoothing Functions</li><ul><li>**moving_average_smooth** - smooths the time series in a dataframe using a moving average with window length defined by ther user</li><li>**bartlett_convolve_smooth** - Smooth all the waveforms in a dataframe using the convolution smoother from [tsmoothie](https://github.com/cerlymarco/tsmoothie) along with a Bartlett window of user-defined length.</li><li>**savgol_smooth** - Smooth all the waveforms in a dataframe using a Savitzky-Golay filter and window of user-defined length.</li></ul></ul><li>At the bottom of the notebook, a sequence of steps using these functions to preprocess the data is demonstrated. The resulting files are what we used as our data for the subsequent clustering attempts. <ul><li>First the time series zeroed at sample detect time and then subset from -30 seconds before sample detect time to 40 seconds after sample detect time. If this full time range is not present in a reading, then that reading is dropped. </li><li> Next, the time series are normalized to scale their values between 0 and 1. </li><li>Finally, convolution smoother with a bartlett window of length 50 is applied to smooth the time series.</li><li> The time series are then written into fresh csv files. </li></ul></ul> |

## **Clustering**

Here are all of the notebooks we used for our clustering attempts for the second iteration, after running the previously described preprocessing steps. 

|Notebook/Script| Description |
|--------|--------------------------------------|
| [PCA](code/pca.ipynb) |This notebook gives an example of running a principle component analysis on the smoothed and normalized data to get visualizations in a lower dimension space. It was our first indication that ECDs might be more separable from unsuccessful after normalization and noise reduction. |
| [KMeans](code/KMeansClustering.ipynb) | This notebook gives an example of running KMeans with euclidean distance to cluster using the filtered time series as predictors along with 3 of the aggregate predictors. It also demonstrates using functions from the [diagnostics.py](Code/diagnostics.py) script to describe the characteristics of the clusters. This is the notebook where we first found a cluster with a decent proportion of ECDs and had most of the unsuccessful readings in the cluster re-identified as ECDs. It gave us hope that other methods might work better.|
| [SOM](code/SOM.ipynb) | This notebook uses a self organizing map (SOM) to try and cluster using the preprocessed waveforms and most important aggregate predictors according to random forest. Of the clustering attempts, these results were the least promising.|
|[RandomForest](code/RandomForest.ipynb) | This notebook performs a random forest using the 'unsuccessful' and 'ECD' labels to find the most important aggregate features.|
| [Autoencoder](code/autoencoder.ipynb) | This notebook uses an autoencoder to derive feautures from the smoothed time series. The top three principal components from running a PCA on these features are then used as predictors along with the most important aggregate features as decided by the random forest. Clustering is performed with Gaussian Mixture Modeling and 21 clusters. The results were very promising, with one cluster having 300 ECDs and only 100 unsuccessful.|
|[KShape](code/KShapeClustering.ipynb)|This notebook uses the [KShape alogrithm](http://www1.cs.columbia.edu/~jopa/Papers/PaparrizosSIGMOD2015.pdf) to cluster using both the smoothed normalized time series and some of the aggregate features as predictors. This notebook also yielded some of the most promising results, with one cluster containing 129 ECD errors and only 8 unsuccessful.|



## **Cluster Evaluation**

|Notebook/Script| Description |
|--------|--------------------------------------|
| [Diagnostics](code/diagnostics.py) |  Script contains the following functions which compute plot information about clusters in hopes of describing them <ul><li>**prepare_data** - gets the data into the proper format for the other diagnostic functions.</li><li>**extract_testid** -Retrieves the test ids in a given cluster and stores them in a dataframe. The user has the option of extracting the ids for a specific type(s) of reading(s) (i.e only the ECDs).</li>  <li>**get_label_counts** - retrieves the number of readings from different categories in each cluster (eg. number of unsuccessful, number of wild ECD errors, etc)</li><li>**describe_clusters** - Prints the number of readings from different categories in each cluster (eg. number of unsuccessful, number of wild ECD errors, etc). Displays histograms for the desired aggregate predictors in each cluster. If show_traces = True, plots the waveforms in each cluster, with one plot for unsuccessful, and one for the various kinds of ECD errors.</li><li>**compare_cluster_densities** - Takes two clusters and a list of numeric aggregate predictors then makes a plot for each predictor comparing the estimated probability densities for the two clusters. This should help give an idea of differeces between clusters on the aggregate predictors, but in practice these plot aren't working too well. </li> </ul>|
