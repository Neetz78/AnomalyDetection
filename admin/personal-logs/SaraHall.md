# Work Log for Sara Hall

| Day   | Hours | Description                              |
|-------|-------|------------------------------------------|
| May02 | 1     |   <ul><li>Set up the Github repo</li><li>Made a Trello account</li><li>read the Capstone expectation slides|</li></ul> |
| May03 | 8     |  <ul><li>Attended the initial class meeting (1.5 hours)</li><li>Met with the client (2 hours) </li><li> Met with the group to create out team contract and team charter and set up our Trello board (2 hours).</li><li>Did some preliminary exploration of the data. (2.5 hours)</ul> |
| May04 | 9     |  <ul><li>Had a meeting with the group and Aditya to figure out more proposal details (1 hour)</li><li>Formatted the meeting minutes from the past two meetings and posted on Github (1 hour) </li><li> Researched time series clustering (1 hour).</li><li> Met with the group to discuss how to split the work for the proposal (1 hour).</li><li>Worked on summarizing the data and developing a timeline for the project. (5 hours)</ul> |
| May05 | 10    | <ul><li>Had another meeting with the group, Aditya, and Levannia to solidify our understanding of the project (1 hour)</li><li>Finsihed writing the data section for the proposal (1 hour) </li><li> Editted and flushed out the introduction and background sections of the proposal. (4 hours).</li><li>Discussed changes with Saisree and worked on literature review together (2 hours).</li><li>Formatted the final paper, fixed references, added some figures (2 hours)<li>Filled in my personal log (5 mins)</ul>                             |
| May06 | 10     |   <ul><li>Read over the proposal and worked on the slides (1.5 hours) </li><li> Had two 15 minute meetings with the team before and after our client meeting (30 mins), had another meeting with the group, Aditya, Levannia, and Mike to get feedback on the proposal (20 mins)</li><li>Wrangled the data - tried to figure  out why not all records are present as both time series and summary statistics. Sent the missing test IDs to the client (4 hours) </li><li> Worked on the proposal slides. (4 hours).</li><li>Filled in my personal log (5 mins)</li></ul>|
| May07 | 0.5     |    <ul><li>Divied up the slides for proposal presentation (30 mins)</li></ul>                                  |
| May08 | 1.5    |       <ul><li>Practiced fiving the proposal presentation (1.5 hours) </li></ul>                                      |
| May09 | 8     |    <ul><li>1 hour exploring the new data </li><li> 1 hour – met with the group to prepare for advisory committee meeting proposal presentation and capstone proposal presentation</li><li> 30 mins – met with Siemens Healthineers advisory committee to give proposal presentation and get feedback </li><li>2.5 hours – did some wrangling to make sure test ids now match properly between the two data sources. Got rid of the pin contact records in the unsuccessful record files so they aren’t duplicated. <li>30 mins – Gave our proposal presentation to teaching staff and met as a group to discuss how it went after. We were a bit disgruntled by the discrepancies in instructions between the slides and Canvas</li></li>2.5 hours – literature review. Started with looking into waveform filtering but decided I need a better understanding of the field so I started reading about enzymatic amperometric biosensor readings. I’m hoping that will give me a stronger domain specific foundation. <li>Filled in my personal log (5 mins)</li></ul>                                        |
| May10 | 8     |     <ul><li>Read more on enzymatic biosensors for background info (2 hours) </li><li>	Met with the group a couple of times to try and organize ourselves better (1 hour)</li><li>	Tried (unsuccessfully) to find literature with what noise reduction strategies have been used for amperometric enzymatic bionsensors in the past (1 hour). </li><li> Explored the time series a bit more in depth. Discovered that between successful, unsuccesful, and pin contact have very different lengths and sample detection times. We will need to find a way to standardize and align.  (2 hours)</li><li>Looked into dynamic time warping, decided we need to figure out some way to normalize (1 hour) </li><li>Looked into normalization. We really need to deal with a lack of temporal alignment in our time series. Also set up a virtual environment with appropriate packages on my computer. (1 hour) </li></ul>                                      |
| May11 | 8     |   <ul><li>Looked into the characteristic of the unsuccessful samples with `sample detection time` at 0.0. They account for around 28% of unsuccessful readings, and after looking at the waveforms, I’m pretty sure 0 indicates that the sample has not been detected, rather than sample detection occurring at 0. (1 hour) </li>  <li>Met with the group to discuss the progress we made yesterday and what we planned on doing today (1 hour).</li>   <li> Read a review paper on the unsupervised whole time-series clustering methods used between 2005 and 2015. Spent some time cross-referencing from their citations (2 hours)</li>   <li> Explained signal frequencies and the general ideas behind filtering and fourier transforms to Saisree and Justine (30 mins)</li>   <li> Played around with normalization/standardization of the waveforms to try and figure out a good way to window them. It looks like the periods leading up to and following sample detection will probably be useful. In terms of Fourier transforms, it might make sense to apply them on separated windows as the noise before/after sample detection differs quite a bit and we may not want to lose that temporal information. (3 hours)</li>   <li> Verified virtual environment versions and organized my local capstone folder structure (30 mins)</li> </ul>|
| May12 | 8     |  <ul> <li> Read through a really good PhD thesis on time series clustering [](https://core.ac.uk/download/pdf/268877028.pdf). They raise some really interesting points about dimensionality reduction, similarity measures, and preserving the information from the initial waveform. I think one of our machine learning pipelines should attempt to mimic their outlined process. (1.5 hours). </li><li> Played around with z-normalizing the time-series and aligning the reading so `SampleDetectTime` is considered time ‘0’. I’m still struggling to figure out how to deal with unsuccessful readings where the sample was never detected. In that regard, it might make more sense to leave time 0 as when the card was inserted into the reader. In that case though, we would need something like dynamic time warping to match the shape of the waveform further along when the sample is detected as this occurs at different times in different readings depending on when the sample was injected. (1 hour). </li><li> Did a bit more reading trying to track down papers similar to the PhD thesis then met with the group to discuss the progress that we’ve been making and brainstorm where to go next. (1.5 hours). </li><li> Found a bug in my initial wrangle notebook when separating pin contacts out from unsuccessful, tried to fix it. Then got an email that we had new data, so I had to re-run the wrangle code and fix it to work with the new data (3 hours). <\li><li> Pulled together some information about what I have accomplished this week to make communicating with Siemens tomorrow a bit easier. Met with the group to discuss (1 hour). <\li> <\ul>|
| May13 | 6     |      <ul> <li>Met with Siemens (Levannia, Mike, Aditya) and the group to tell them what progress we’ve made this week and ask them our questions (1 hour)</li><li>Met with the group to discuss our next steps (30 mins)</li><li>Finished writing out the meeting minutes for this morning, and summarized how I’ve spent my time this week for next week’s MDS check-in (30 mins)</li><li>Wrote a script to standardize and remove the wet-up period from all of the time series (2 hours)</li><li>Contemplated different strategies to deal with the shorter time series. Three of the pin contact series had no data left after removing the wet-up period, same with around 30% of the unsuccessful series. (1 hour)</li><li>Played around with making a distance matrix using dynamic time warping for the unsuccessful and pin contact series. So far, I have been unsuccessful as it keeps eating up all my computer CPU. I’m still concerned about issues with trying to compare two series where one is shut off long before the other. I believe and assumption of DTW is that the two signals start and end in the same ‘place’. (1 hours)</li></ul>                                    |
| May15 | 1     |    Discussed project next steps with Justine (1 hour)                    |
| May16 | 9     |    <ul><li>Met with the group to discuss what we needed to go over in our Monday morning with Siemens (15 mins)</li><li>Met with Siemens to discuss our plan for the week (30 mins) </li><li>Met with the group to further subdivide tasks (15 mins) </li><li>Debugged my code to run MDS with DTW on the whole time-series following wet-up and plotted it colour-coded based on pin-contact vs. unsuccessful. The results were not promising (1 hour). </li><li>Tried MDS on only the first 20 seconds of the readings. Results were still bad (30 mins). </li><li>Worked with the time-series to figure out the most appropriate way to window and which unsuccessful readings are appropriate to get rid of (1.5 hours). </li><li>Wrote code to split all the time series into different windows based on sample detect time. Decided to drop readings with NAs in these periods for now to simplify things. Might come back to it later (4 hours). </li><li>Prepared for our weekly presentation tomorrow (1 hour) </li></ul>                                      |
| May17 | 8.5     |   <ul><li>Met with the group to prepare for weekly update presentation (30 mins)</li><li>Spent some time cleaning up the Gtihub repo (30 mins) )</li><li>Met with Saisree to talk about outliers in the unsuccessful time series (30 mins) )</li><li>Updated my preprocessing code to deal with the new synthetic pin contact readings; thought about how to organize things into modular scripts moving forward (2 hours) )</li><li>Played around with getting MDS to work on all the data from the calibration period. It was taking forever to execute in Python so I exported the distance matrix and ran it in R. The 2D representation of the time series did not show a separation between pin contact and unsuccessful (1 hour). )</li><li>Read through the tslearn package documentation and installed it in my capstone virtual environment. Had to troubleshoot some issues I was having with my virtual environment (1 hour). )</li><li>Tried several of the clustering possibilities provided by tslearn on the calibration window. Results were not great (1 hours). )</li><li>Tried running PCA on the time series to see if I can get exemplars of where most of the variation occurs (1 hour). )</li><li>Went back to fix up my notebooks from the day and found that I’m having conflicts with the versions of numpy required by different packages I installed. Troubleshooted this, unsuccessfully. Gave up and made a new virtual environment with fresh package installs (30 mins). )</li><li>Went back to get MDS to work on the calibration waveforms. To make it faster, took a random sample of 2000. It’s still taking forever to finish running. (30 mins). )</li></ul>                                       |
| May18 | 9     |   <ul><li>Ran MDS on DTW pairwise matrices once for each of the different windows. Plotting the results revealed that at least in two dimensions, there are no clusters forming from the raw waveforms. I don’t think DTW is going to yield very good results for us. (30 mins) </li><li>Played around with the clustering on tslearn – I don’t think the package is going to be very helpful for us moving forward (2 hours). </li><li>Created new data frames with real fast Fourier transform representations of the time series. Visually they look pretty much identical between pin contacts and unsuccessful, but we can still try and cluster on them moving forward to see if they are useful (1 hour). </li><li>Looked into setting up an autoencoder for feature extraction. So far I have been unsuccessful, but have discovered that the sequitar package is awful and full of bugs so we’ll have to do it by hand if that’s what we choose to do. Also, even after getting a reduced dimension from an autoencoder, we would need to find a way to perform clustering. Almost everything I’ve read on the internet has used labels for it (4 hours). </li><li>Got frustrated and went back to reading about different unsupervised methods for time series clustering on the internet. Potential idea for tomorrow: clustering based on autocorrelation? https://medium.com/wwblog/time-series-clustering-based-on-autocorrelation-using-python-94d5e3475179  (1 hour) </li><li>Looked at Justine’s weird mixture model results (30 mins) </li></ul>                                       |
| May 19 | 9  | <ul><li>Met with the group to catch up (30 mins). </li><li>Worked with Justine’s PCA on the predictors file trying to help figure out what was wrong with it. Sadly,we came to the realization that there was in fact a bug, and the initial clusters she got were a fluke (4 hours). </li><li>Tried running mixture models and hclust on the resulting waveforms from discrete fourier transform. Had no luck (2 hours). </li><li>Tried to organize some of my code (1 hour). </li><li>Met with the group to discuss what we want to talk about with the client tomorrow (30 mins). </li><li>Worked on getting some slides together and figuring out what we should talk about in next week’s meeting with the Siemen’s advisory committee (1 hour). </li></ul>|
| May 20| 6     | <ul><li>Met with the group to discuss where we’re at and prepare for our client meeting (30 mins)</li><li>Met with the client to discuss progress ( 1 hour) </li><li>Organized my file directory and thought about how we should organize our github moving forward. I’m thinking a glossary with description and links to notebooks of all our machine learning attempts. (1 hour) </li><li>Redid my windowing notebook to window before standardization and reran my PCA on the raw waveforms. Results were not promising (2 hours) </li><li>Did administrative organization and got slides ready for the three meetings we have on Tuesday (1.5 hours). </li></ul> |
  | May 23|  8     | <ul><li>Met with the group to discuss where we're at (15 mins)</li><li> Adminstrative work - finished updating the slides for tomrrow's presentations, made a spreadsheet to keep an organized record of our clustering attempts (75 mins)</li> <li>Worked on looking at the calibration window before stnadardization and clustering. (2.5 hours) </li><li> Looked into new potential methods as our current attempts are not going well (2 hours). </li><li>Met in person with the team to discuss where we're at and get ready for the 1500 presentations we're giving tomorrow (2 hours)</li></ul>|
| May 24| 10    | <ul><li>Looked at comparing the traces between unsuccessful and pin contact errors to try and figure out what the differences are (unsuccessfully – they pretty much look the same) (2 hours)</li><li>Met with our client (Levannia and Aditya) to discuss progress and brainstorm (80 mins) </li><li>Worked on recording detailed meeting minutes to keep track of what was said in our meeting ( 40 mins) </li><li>Met with the Siemens advisory committee to present our progress so far (1 hour) </li><li>Had an impromptu meeting with Justine, Levannia and Aditya to discuss switching our focus from clustering to noise reduction (30 mins) </li><li>Met with Debangsha and Irene for our weekly check-in (30 mins) </li><li>Met with the team to discuss what we need to do over the next few days to make progress with our project and get ready for the midterm presentation next week (1 hour) </li><li>Worked on running fourier transforms on the pin contact reading for our analyte and for a ‘cleaner’ one, for which they gave us the data for today. My hope is that by characterizing what frequencies are more prevalent in our analyte but not the clean one we will get a better idea of which frequencies need to be filtered out ( 3 hours) </li></ul> |
| May 25| 8.5   | <ul><li>Read more into filtering methods (1 hour)</li><li>Explored the data and tried to characterise what ‘noise’ is for us (1 hour) </li><li>Played around with trying to figure out the optimal parameters for Savitzky-Golay FIR filter and computed power density spectrums for different windows to see if that might be helpful (5.5 hours) </li><li>Met with the team to discuss the progress (or lack-thereof) that we’ve made with filtering (1 hour) </li></ul>|
| May 26| 10.5  | <ul><li>Made a cleaned up notebook with noise filtering and played around with the parameters for Savgoy filtering (6 hours) </li><li>Met with the team to discuss progress (1 hour)</li><li>Debugged Justine’s window function (1.5 hours) </li><li>Taught Justine about Fourier transforms and spectral density. Walked through a synthetic example in Python (1 hour) </li><li>Made some summary slides so we have a clear way to present our progress to our client tomorrow (1 hour) </li></ul> |
| May 27| 10    | <ul><li>Met with the team to discuss where we were all at and get ready for out client meeting (30 mins)</li><li>Met with Levannia and Aditya to show them the progress we have made with filtering (30 mins) </li><li>Made a cleaned-up notebook to window, normalize, and perform convolutional smoothing on all of the waveforms. It’s ready to get converted into script should that be what we choose to go with (4 hours) </li><li>Ran PCAs on the newly filtered waveforms and looked at the results (2 hours) </li><li>Performed feature extraction on the filtered waveforms using TSFresh and performed some clusterin, then looked at the results. Performance is better than before, but still not great (1 hours) </li><li>Tried running some whole clustering with DTW, but the algorithm is taking forever to run (1 hour) </li><li>Worked on making some slides for our presentation to the data review team at Siemens next week (1 hour) </li></ul> |
| May 28| 3     | Worked on the slides for the data review meeting with Siemens on Monday |
| May 29 | 1    | Practiced my slides for the data review meeting. |
| May 30 | 10   | <ul><li> Met with the team to get ready for our data review presentation (30 mins) </li><li>Tried to figure out what was going wrong with the TSFRESH subclustering last night. It looks like ‘important’ clusters aren’t getting pulled out. Regardless, the results are bad. (1 hour) </li><li>Gave our presentation to the Siemens data review team (30 mins) </li><li>Met with the team to rehearse our midterm presentation and finalize our slides for tomorrow (4 hours) </li><li>Ran a kmeans model on the filtered and normalized data. Cleaned up some of my other code while it was running (2 hours) </li><li>Wrote some code to graph summary information about the clusters I got from kmeans. It definitely looks like they are clustering based on shape which is cool! Then played around with using Euclidean instead of DTW for distance. Results were surprisingly not terrible (2 hours) </li></ul>|
 | May 31 | 9   | <ul><li> Met with the group to do some last minute prep for our midterm presentation (0.5 hours)</li><li>Attended the class midterm presentations and gave ours (3.5 hours) )</li><li>Played around with various different ways of clustering the whole waveforms and looked at the clusters to try and figure out what’s getting grouped together and how many groups might be optimal (5 hours) )</li></ul>|
 | ----- | ----- | -------------- End of May -------------- |
| Jun01 | 8    |     <ul><li> Did some quick code cleanup in prep for our meeting (30 mins) </li><li>Met with Aditya, Mike, and Levannia for a code review and to update them on our progress (1 hour) </li><li>Captured detailed meeting minutes (1 hour) </li><li>Worked on trying different things to see how the clusters change and if I could improve performance  (5 hours) </li><li>Started reading into ways of competing clustering models so we can try and figure out when we’re getting the ‘good’ clusters (30 mins) </li></ul>                                     |
| Jun02 | 8     |     <ul><li> Played around with different parameters for kmeans clustering (2 hours) </li><li>Made a cleaned up and well documented notebook with all of the preprocessing functions and a demo of how to use them.  (4 hours) </li><li>Debugged why I suddenly couldn’t get the same clusters as yesterday even though I had the seed set. It turns out I ran my cells in a weird order and was accidentally clustering on a column with the clusters from a previous attempt (2 hours) </li></ul>                                     |
| Jun03 | 8     |     <ul><li> Weekly check-in with Mike, Aditya, and Levania (30 mins)</li> <li>Met with the team to discuss progress (30 mins) </li><li>Worked on writing/documenting code to describe clusters (7 hours)</li></ul>                                     |
| Jun06 | 9    |   <ul> <li>Looked over the slides for our Siemens advisory minutes then worked on cleaning up the preprocessing notebook (1 hour) </li><li>Met with Saisree, Justine, and Neethu to practice our presentation (30 mins) </li><li>Worked on cleaning up the diagnostic functions in the KMeans notebook and making them more usable (3 hours) </li><li>Met with the Siemens advisory committee and gave them our progress update (30 mins) </li><li>Edited the presentation for the MDS check in with Irene tomorrow and made sure our slides meet all of the requirements (1 hour) </li><li>Moved diagnostic functions into a script so that they can be imported and used by different clustering notebooks (1 hour) </li><li>Tried re-downloading the data and running the preprocessing from scratch to make sure everything is still working after all the code cleaning. Had to do some debugging to get everything working.  (2 hours) </li><li>Cleaned up and documented the PCA notebook (1 hour) </li></ul>                                       |
| Jun07 | 8     |   <ul><li> Made and tested some new functions to describe the clusters that are obtained from our pipelines (5 hours) </li><li>Met with the team to prep for our weekly check in with Irene and Debangsha (30 mins) </li><li>Gave our weekly update to Irene and Debangsha (30 mins) </li><li>Pulled all of my preprocessing notebooks from the first iteration into a file, added clear descriptions to them, and made a glossary showing the order to execute them and describing what they do. These are going in archive since they yielded bad results but our client still wants a record of them. (2 hours) </li></ul>                                       |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| ----- | ----- | -------------- End of June ------------- |

