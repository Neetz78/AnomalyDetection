# Work Log for Sara Hall

| Day   | Hours | Description                              |
|-------|-------|------------------------------------------|
| May02 | 1     |   <ul><li>Set up the Github repo</li><li>Made a Trello account</li><li>read the Capstone expectation slides|</li></ul> |
| May03 | 8     |  <ul><li>Attended the initial class meeting (1.5 hours)</li><li>Met with the client (2 hours) </li><li> Met with the group to create out team contract and team charter and set up our Trello board (2 hours).</li><li>Did some preliminary exploration of the data. (2.5 hours)</ul> |
| May04 | 9     |  <ul><li>Had a meeting with the group and Aditya to figure out more proposal details (1 hour)</li><li>Formatted the meeting minutes from the past two meetings and posted on Github (1 hour) </li><li> Researched time series clustering (1 hour).</li><li> Met with the group to discuss how to split the work for the proposal (1 hour).</li><li>Worked on summarizing the data and developing a timeline for the project. (5 hours)</ul> |
| May05 | 10    | <ul><li>Had another meeting with the group, Aditya, and Levannia to solidify our understanding of the project (1 hour)</li><li>Finsihed writing the data section for the proposal (1 hour) </li><li> Editted and flushed out the introduction and background sections of the proposal. (4 hours).</li><li>Discussed changes with Saisree and worked on literature review together (2 hours).</li><li>Formatted the final paper, fixed references, added some figures (2 hours)<li>Filled in my personal log (5 mins)</ul>                             |
| May06 | 10     |   <ul><li>Read over the proposal and worked on the slides (1.5 hours) </li><li> Had two 15 minute meetings with the team before and after our client meeting (30 mins), had another meeting with the group, Aditya, Levannia, and Mike to get feedback on the proposal (20 mins)</li><li>Wrangled the data - tried to figure  out why not all records are present as both time series and summary statistics. Sent the missing test IDs to the client (4 hours) </li><li> Worked on the proposal slides. (4 hours).</li><li>Filled in my personal log (5 mins)</li></ul>|
| May07 | 0.5     |    <ul><li>Divied up the slides for proposal presentation (30 mins)</li></ul>                                  |
| May08 | 1.5    |       <ul><li>Practiced fiving the proposal presentation (1.5 hours) </li></ul>                                      |
| May09 | 8     |    <ul><li>1 hour exploring the new data </li><li> 1 hour – met with the group to prepare for advisory committee meeting proposal presentation and capstone proposal presentation</li><li> 30 mins – met with Siemens Healthineers advisory committee to give proposal presentation and get feedback </li><li>2.5 hours – did some wrangling to make sure test ids now match properly between the two data sources. Got rid of the pin contact records in the unsuccessful record files so they aren’t duplicated. <li>30 mins – Gave our proposal presentation to teaching staff and met as a group to discuss how it went after. We were a bit disgruntled by the discrepancies in instructions between the slides and Canvas</li></li>2.5 hours – literature review. Started with looking into waveform filtering but decided I need a better understanding of the field so I started reading about enzymatic amperometric biosensor readings. I’m hoping that will give me a stronger domain specific foundation. <li>Filled in my personal log (5 mins)</li></ul>                                        |
| May10 | 8     |     <ul><li>Read more on enzymatic biosensors for background info (2 hours) </li><li>	Met with the group a couple of times to try and organize ourselves better (1 hour)</li><li>	Tried (unsuccessfully) to find literature with what noise reduction strategies have been used for amperometric enzymatic bionsensors in the past (1 hour). </li><li> Explored the time series a bit more in depth. Discovered that between successful, unsuccesful, and pin contact have very different lengths and sample detection times. We will need to find a way to standardize and align.  (2 hours)</li><li>Looked into dynamic time warping, decided we need to figure out some way to normalize (1 hour) </li><li>Looked into normalization. We really need to deal with a lack of temporal alignment in our time series. Also set up a virtual environment with appropriate packages on my computer. (1 hour) </li></ul>                                      |
| May11 | 8     |   <ul><li>Looked into the characteristic of the unsuccessful samples with `sample detection time` at 0.0. They account for around 28% of unsuccessful readings, and after looking at the waveforms, I’m pretty sure 0 indicates that the sample has not been detected, rather than sample detection occurring at 0. (1 hour) </li>  <li>Met with the group to discuss the progress we made yesterday and what we planned on doing today (1 hour).</li>   <li> Read a review paper on the unsupervised whole time-series clustering methods used between 2005 and 2015. Spent some time cross-referencing from their citations (2 hours)</li>   <li> Explained signal frequencies and the general ideas behind filtering and fourier transforms to Saisree and Justine (30 mins)</li>   <li> Played around with normalization/standardization of the waveforms to try and figure out a good way to window them. It looks like the periods leading up to and following sample detection will probably be useful. In terms of Fourier transforms, it might make sense to apply them on separated windows as the noise before/after sample detection differs quite a bit and we may not want to lose that temporal information. (3 hours)</li>   <li> Verified virtual environment versions and organized my local capstone folder structure (30 mins)</li> </ul>|
| May12 | 8     |  <ul> <li> Read through a really good PhD thesis on time series clustering [](https://core.ac.uk/download/pdf/268877028.pdf). They raise some really interesting points about dimensionality reduction, similarity measures, and preserving the information from the initial waveform. I think one of our machine learning pipelines should attempt to mimic their outlined process. (1.5 hours). </li><li> Played around with z-normalizing the time-series and aligning the reading so `SampleDetectTime` is considered time ‘0’. I’m still struggling to figure out how to deal with unsuccessful readings where the sample was never detected. In that regard, it might make more sense to leave time 0 as when the card was inserted into the reader. In that case though, we would need something like dynamic time warping to match the shape of the waveform further along when the sample is detected as this occurs at different times in different readings depending on when the sample was injected. (1 hour). </li><li> Did a bit more reading trying to track down papers similar to the PhD thesis then met with the group to discuss the progress that we’ve been making and brainstorm where to go next. (1.5 hours). </li><li> Found a bug in my initial wrangle notebook when separating pin contacts out from unsuccessful, tried to fix it. Then got an email that we had new data, so I had to re-run the wrangle code and fix it to work with the new data (3 hours). <\li><li> Pulled together some information about what I have accomplished this week to make communicating with Siemens tomorrow a bit easier. Met with the group to discuss (1 hour). <\li> <\ul>|
| May13 | 6     |      <ul> <li>Met with Siemens (Levannia, Mike, Aditya) and the group to tell them what progress we’ve made this week and ask them our questions (1 hour)</li><li>Met with the group to discuss our next steps (30 mins)</li><li>Finished writing out the meeting minutes for this morning, and summarized how I’ve spent my time this week for next week’s MDS check-in (30 mins)</li><li>Wrote a script to standardize and remove the wet-up period from all of the time series (2 hours)</li><li>Contemplated different strategies to deal with the shorter time series. Three of the pin contact series had no data left after removing the wet-up period, same with around 30% of the unsuccessful series. (1 hour)</li><li>Played around with making a distance matrix using dynamic time warping for the unsuccessful and pin contact series. So far, I have been unsuccessful as it keeps eating up all my computer CPU. I’m still concerned about issues with trying to compare two series where one is shut off long before the other. I believe and assumption of DTW is that the two signals start and end in the same ‘place’. (1 hours)</li></ul>                                    |
| May15 | 1     |    Discussed project next steps with Justine (1 hour)                    |
| May16 | 9     |    <ul><li>Met with the group to discuss what we needed to go over in our Monday morning with Siemens (15 mins)</li><li>Met with Siemens to discuss our plan for the week (30 mins) </li><li>Met with the group to further subdivide tasks (15 mins) </li><li>Debugged my code to run MDS with DTW on the whole time-series following wet-up and plotted it colour-coded based on pin-contact vs. unsuccessful. The results were not promising (1 hour). </li><li>Tried MDS on only the first 20 seconds of the readings. Results were still bad (30 mins). </li><li>Worked with the time-series to figure out the most appropriate way to window and which unsuccessful readings are appropriate to get rid of (1.5 hours). </li><li>Wrote code to split all the time series into different windows based on sample detect time. Decided to drop readings with NAs in these periods for now to simplify things. Might come back to it later (4 hours). </li><li>Prepared for our weekly presentation tomorrow (1 hour) </li></ul>                                      |
| May17 | 8.5     |   <ul><li>Met with the group to prepare for weekly update presentation (30 mins)</li><li>Spent some time cleaning up the Gtihub repo (30 mins) )</li><li>Met with Saisree to talk about outliers in the unsuccessful time series (30 mins) )</li><li>Updated my preprocessing code to deal with the new synthetic pin contact readings; thought about how to organize things into modular scripts moving forward (2 hours) )</li><li>Played around with getting MDS to work on all the data from the calibration period. It was taking forever to execute in Python so I exported the distance matrix and ran it in R. The 2D representation of the time series did not show a separation between pin contact and unsuccessful (1 hour). )</li><li>Read through the tslearn package documentation and installed it in my capstone virtual environment. Had to troubleshoot some issues I was having with my virtual environment (1 hour). )</li><li>Tried several of the clustering possibilities provided by tslearn on the calibration window. Results were not great (1 hours). )</li><li>Tried running PCA on the time series to see if I can get exemplars of where most of the variation occurs (1 hour). )</li><li>Went back to fix up my notebooks from the day and found that I’m having conflicts with the versions of numpy required by different packages I installed. Troubleshooted this, unsuccessfully. Gave up and made a new virtual environment with fresh package installs (30 mins). )</li><li>Went back to get MDS to work on the calibration waveforms. To make it faster, took a random sample of 2000. It’s still taking forever to finish running. (30 mins). )</li></ul>                                       |
| May18 | 9     |   <ul><li>Ran MDS on DTW pairwise matrices once for each of the different windows. Plotting the results revealed that at least in two dimensions, there are no clusters forming from the raw waveforms. I don’t think DTW is going to yield very good results for us. (30 mins) </li><li>Played around with the clustering on tslearn – I don’t think the package is going to be very helpful for us moving forward (2 hours). </li><li>Created new data frames with real fast Fourier transform representations of the time series. Visually they look pretty much identical between pin contacts and unsuccessful, but we can still try and cluster on them moving forward to see if they are useful (1 hour). </li><li>Looked into setting up an autoencoder for feature extraction. So far I have been unsuccessful, but have discovered that the sequitar package is awful and full of bugs so we’ll have to do it by hand if that’s what we choose to do. Also, even after getting a reduced dimension from an autoencoder, we would need to find a way to perform clustering. Almost everything I’ve read on the internet has used labels for it (4 hours). </li><li>Got frustrated and went back to reading about different unsupervised methods for time series clustering on the internet. Potential idea for tomorrow: clustering based on autocorrelation? https://medium.com/wwblog/time-series-clustering-based-on-autocorrelation-using-python-94d5e3475179  (1 hour) </li><li>Looked at Justine’s weird mixture model results </li><li></ul>                                       |
| ----- | ----- | -------------- End of May -------------- |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| ----- | ----- | -------------- End of June ------------- |

