# Work Log for Justine Filion

| Day   | Hours | Description                              |
|-------|-------|------------------------------------------|
| May 01 | X     |                                          |
| May 02 | X     |                                          |
| May 03 | 8     | <ul><li>Attended class where the format of the class was introduced.</li><li>Had a 2 hour meeting with our client to discuss the project.</li><li>Had a 2 hour meeting with the group to complete the team contract, the code of conduct and the team charter canvas provided by the client.</li><li>Set up the trello board and made a list of what we had to complete. Added all the upcoming meetings to the common calendar.</li><li>Explored the data and documentation that was given to us for approximately 2 hours.</li></ul>                                 |
| May 04 | 9    |  <ul><li> Had a 30 minute meeting with Aditya to go over the questions we had from the previous day.</li><li>Right after, we had another 30 minute meeting with the team to discuss the next steps.</li><li> Spent the following 2 hours, looking at documentation on time series clustering.</li><li> Had another 30 minute meeting with the team after lunch to discuss the proposal and how to split the workload.</li><li>Spent from 2 to 7 trying to clarify the scope of the project by researching further information on anomaly detection and writing the first draft of our timeline for the proposal.</li>                                        |
| May 05 | 9     |  <ul><li>Had a 1 hour meeting with Aditya and Levannia to address the questions we had about how to approach the unsupervised techniques and others</li><li>Wrote the minutes for the meeting, which led me to do some research to fill some gaps in my understanding (2 hours)</li><li>Completed the deliverable, schedule and responsabilities section (2 hours)</li><li>Had a meeting with Neethu where we wrote the 'Aims and objectives' section of the proposal (2 hours)</li><li>Started working on the powerpoint presentation for our meeting with the advisory commitee and finished the formatting of the proposal (2 hours)</li>                                       |
| May 06 | 6     |  <ul><li>Had a 15 minute meeting with the team before the meeting with the client (20 minutes), followed by another team meeting after (15 minutes).</li><li>Worked on the powerpoint presentations (for advisory committee and capstone advisors) for the next 5 hours.</li>                                          |
| May 08 | 3     | <ul><li>Spent approximately 2 hours reading literature on imbalanced datasets</li><li>Spent 30 minutes practicing and modifying slides for the advisory committee presentation</li><li>Spent 30 minutes practicing presentation with all team members</li>                                        |
| May 09 |  8    |  <ul><li> Spent the first 2 hours of the morning continuing to read on imbalanced datasets (Synthetic Minority Oversampling Technique, Tomek links, Sample subset optimization)</li><li> Had a 45 minutes meeting with the team to go over the slides and practice for our 2 presentations (advisory committee and MDS check-in)<\li><li> Had out first advisory committee that lasted app. 45 minutes followed by a 15 min team debrief <\li> Spent 1 hour trying to help Sara find the discrepancies between the time series data and the predictor data (to make sure the same IDs were in both with no duplicates) </li><li> Spent another 2 hours reading various research paper looking at different pipelines to deal with imbalanced data (i.e. combine over and undersampling, combine SMOTE + tomek links, condensed nearest neighbor rule with tomek links = one-sided selection)</li><li> Looked into how to set up virtual environments and include a .gitignore file in the repo (also decided how to deal with the requirements.txt file (45 minutes)</li><li> (30 minutes) Wrote the minutes for the MDS check-in meeting and read a paper on anomaly detection in ECG waveforms</li>                                      |
| May 10 | 8     | <ul><li> Met with the team a couple of times throughout the day to try and organize ourselves better (1 hour)</li><li> Researched different statistical tests we could use to obtain a representative sample from the successful readings (to decrease imbalanced dataset --> maybe go from 400 000 successful readings to 10 000) (4 hours).</li><li> Spent 2 hours implementing the two-sample Kolmogorov-Smirnov Test for our aggreagare predictors data to compares samples (the subset vs total readings) to see if their distribution matched.</li><li> Spent 1 hour creating my virtual environment and trying to understand why the version of the packages that I installed did not correspond to those in my requirements.txt file.                                          |
| May 11 | 8    |  <ul><li> Had a meeting with the team to discuss the progress we made yesterday and what we planned on doing today (1 hour)</li><li> Spent 3 hours reading a review paper on what has been done from 2005-2015 in terms of time-series clustering. This led me to do a lot of other research on various techniques/vocabulary I was not familiar with</li><li>Read various litterature on algorithms used on clustering of time series with big data (most of the research I had seen used small datasets. With over 400 000 timeseries, calculating distance matrices using dynamic time warping, for example, is inconceivable. We need to find a way to either decrease our sample size or find different algorithms) (2 hours)</li><li> Looked into oversampling techniques specific to timeseries data, because we will have to find a way to increase or 80 pin contact errors somehow - still don't have a clear idea on how to do it (1 hour)</li><li>Tried to get a better understanding of filtering and Fourier transformation (sara + 3blue1brown videos).(1 hour)</li>                                           |
| May 12 | 7    | <ul><li> Read various papers to find a way to sample a subset of the successful readings..again (and still found no research where they used a similar technique). I was able to make an important distinction in the kind of sampling that I do want, and found that most paper talk about sequence sampling (oversampling and undersampling) which refers to sampling within a specific time sample. This is not what we want. Instead, we want to artificially make new complete timeseries that mimic pin contact errors. I don't think this will work. (2 hour) </li><li> Continued to read more about bootstrapping and how it could possibly help us with our imbalanced data. Found no literature where they used a similar approach to ensemble classification but this time for clustering (i.e bagging or random forest where we apply the same algorithm to different data and aggregate the results). I found somehting that seemed similar (consensus clustering), but here, they use the clusters obtainedfrom various different models applied to the same data and come up with the best clustering. This will not work in our case, we need the same algorithm applied to different data (2 hours).</li><li> Spent 1 hour making a big review of what I have researched, what I have tried and what i think will work and not work (mainly no work, still have not found a solution).</li><li>Read excerpts from a PhD thesis on whole timeseries clustering. There was a lot of different algorithms/pipelines used and they specified the difference between a feature based approaches versus shape-based. We may potentially want to split the teams so that 2 memebers look at each approach, versus using the predictor file.(1 hour) </li><li> Spent 1 hour in total in team meetings (30 minutes this morning to catch-up and 30 minutes at the end of the day to go over what we want to present during tomorrows meeting with Siemens)</li>.                                          |
| May 13 | 6.5  | <ul><li>Met with the client to discuss about the progress that was made this week (1 hour)</li><li>Met with the group to discuss next steps (30 mins)</li><li>Made the presentation for next weeks meeting with our capstone adviser (1 hour)</li><li> Redownloaded all the data files (new files received from client) and ran Sara's script to standardize them. Organized my folders for the project. (1 hour)</li><li>Had a 20 minute meeting with Irene to discuss clustering interrogations</li><li>Started writing a function to perform random sampling from our data to be used for undersampling (1.5 hours).<\li><li>Met with Jeff to discuss clustering interrogations (1 hour).</li>                                          |
| May 14 | 1     | <ul><li>Calculated features using the tsfresh python package</li></ul>                                        |
| May 15 | 1     | <ul><li>Discussed with Sara what had to be adressed in Mondays meeting with Siemens. Went over the assumptions of DTW and discussed why we don't think it is an appropriate measure.</li></ul>                                         |
| May 16 | 9    | <ul><li>Had a meeting to go over what Sara and I had planned on saying to Siemens with the others. They were good with it and had nothing to add.(15 minutes)</li><li>Had a meeting with Siemens to go over what we had planned for this week (30 minutes)</li><li>Had another team meeting to discuss plans for the day (15 min).</li><li> Used the features that I had calculated Saturday to try and do some clustering. (2 hours). Obtained very poor results.</li><li>Read some literature of different features to extract from timeseries. Read more on Fourier transforms. (1 hour).</li><li>Found the `extract_features` function in the tsfresh package that calculates app. 600 features for timeseries (much more than I had done on Saturday, manually). Had to wrangle the data in a format that could be used by the function.Also applied the `select_features` function to filter the features according to which ones were most relevant to classify pin contact and unsuccessful (2.5 hours).</li><li>Scaled the features and applied hierarchical clustering. Found the confusion matrix. Results arent good. (1.5 hours).</li><li> Worked on the presentation for tomorrows meeting (1 hour).</li>                                          |
| May 17 | 6.5     | <ul><li>Fixed the slides and practiced my part(30 min).</li><li>Meeting with team to go over presentation for today (15 min)</li><li> Cleaned the git repo (15 min)</li><li>Had to standardize data (after modification in script). Debugged an error with the script.(30 min)</li><li>Windowed the data with the new standardized data (30 min)</li><li> Meeting with Irene and Debangsha (30 min)</li><li>Downloaded new synthetic pin contact errors, standardized and windowed (30 minutes)</li><li>Wrote a function that creates features matrices from the windowed data (30 min).</li><li>Performed PCA on the feature matrix obtained from the calibration window. Plotted the first and second component and obtained results that were promising. There seemed to be 2 groups of pin contact errors, ones with low scores on PC1 and PC2 and the other with high scores on PC1. Had to go over PCA notes to understand difference between scores and loadings. Created scree plot and cumulative proportion of variance to choose number of components to keep. (2.5 hours)</li><li> Used hierarchical clustering on the first 5 principal components. Did not get good results.(30 minutes)</li></ul>|
| May 18 | 9    | <ul><li> Had a meeting with the team to discuss what we did yesterday. (30 min)</li><li>Continued working on clustering PCA components obtained from the features on the calibration window. Did not get good results. (30 minute)</li><li> Applied the same PCA technique to the post window (1.5 hour).</li><li> Had a meeting with Debangsha. We discussed potential ideas (20 minutes). </li><li> Found that using the post window, if I used a multi-step clustering approach on the principal components, I get VERY good results! Spent all day trying to make sure there was no bug in my code and that the good results weren't a fluke. Cleaned my notebook so that'd be easier to present to the other teamates tomorrow. (6 hours)</li><ul>                                         |
| May 19 | 9   | <ul><li> Had a meeting with the team (30 minutes). </li> <li>Continued working on the clustering from yesterday and found a coding error that resulted in me losing all the good results :(. I thus had to start from scratch and go back to look at each window individually and see if I could find any clusters. I did not. (5 hours)</li><li> Went back and windowed the data before scaling thinking maybe that scaling first removes important information that could be useful for the clustering (2 hours).</li><li>Had a meeting with the team to discuss tomorrow's meeting with the client and worked on the powerpoint slides (1h30).</li></ul>                                      |
| May 20  | 5   | <ul><li> Had a meeting witht the team to go over the slides for our meeting with Siemens (30 min).</li><li>Had a meeting with Siemens where we discussed what we did this week. We got a lot of feedback and some good ideas on what to explore next (1 hour).</li><li> Wrote the weekly update (15 min).</li><li> Debugged why my feature extraction function wasn't working on the data that had been windowed and then standardized. Windowing first results in a std that is very close to 0 in the calibration window and so the standardized waveforms explode (divide by very small number) (1.5 hour)</li><li> After discussing with Levannia, she mentionned that standardizing the calibration might not be necessary because the same fluid type is used. Also, calibration period is where most of the pin contacts occur. Tried extracting features from unstandardized waveforms for calibration window and then clustering. No significant results were obtained. Also, increase the number of clusters to 31 (which equates to the number of different returncode (after suggestion from Levannia) (2 hour)                                      |
| May 23  |  8   | <ul><li>Meeting with the team (15 min)</li><li>Created plan for directory so that everyone has the same structure and paths (15 min)</li><li>Created 3d plot to visualize the principal components differently (1.5 hours)</li><li>Made sure that all notebooks (for each window) contained the updated version of the feature extraction function. Cleaned up all the notebooks and kept only the ones that were useful. Changed the format of the dataframe returned by the feature_matrix  function (2 hours)</li><li>Working on presentation slides for tomorrow (20 min)</li><li>Worked on creating a template for the tsfresh feature extraction - clustering pipeline to include in the github (2 hours)</li><li> Meeting with the team to go over the 3 presentations we have tomorrow (2 hours)</li> |
| May 24 |  9  | <ul><li> Had a meeting with Siemens (1h20min)</li><li> Went over the slides for the presentation with Irene (40 min)</li><li>Meeting with Irene and Debangsha (30 min)</li><li>Wrote the minutes for the meeting (30 min) </li><li> Had the advisory comittee meeting (1 hour)</li><li>Had an impromptu meeting with Levannia and Aditya to go over some suggestions they had (30 min)</li><li> Sent some files to Levannia so that she could give us data on a 'target' analyte (20 min).</li><li>Had a meeting with the team members to organize ourselves (next steps) (40 mins).</li><li> Wrote the minutes for the impromptu meeting (30 min)</li><li>Finished the notebook to give Siemens for the tsfresh pipeline (3 hours) </li>                                      |
| May 25 |  8   | <ul><li> Read on the different types of filtering (more specifically Savitzkyâ€“Golay filter and median filter) and played with them and their parameters in python to see the effect they had on our waveforms. Compared the filtered waveforms to our target analyte (that is less noisy then ours) to determine how much noise to remove. Found it difficult to compare them as they are on very different scales. Read into possible ways to determine the optimal parameters instead. (3.5 hours).</li><li> Had a meeting with the team to talk about the different filtering technique we had looked at. (1 hour).</li><li> Continued exploring the waveforms and decided to just try a very simple rolling average for now and see what kind of results we get from that. Built 2 python functions, 1 that plots the rolling mean overtop the raw data and the other, that create the rolling average dataframe. (3.5 hours).                                       |
| May  |     | <ul><li>                                       |
| May  |     | <ul><li>                                       |
| May  |     | <ul><li>                                       |
| May  |     | <ul><li>                                       |  
| ----- | ----- | -------------- End of May -------------- |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| Jun0X | X     |                                          |
| ----- | ----- | -------------- End of June ------------- |

